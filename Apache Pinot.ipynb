{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optional-titanium",
   "metadata": {},
   "source": [
    "# Apache Pinot\n",
    "\n",
    "_Seminararbeit von Nikola Braukm√ºller und Tim Ebert zum Modul \"Advanced Data Management\" (W3M20011) am DHBW CAS (WiSe 2020/21)._  \n",
    "_Abgabedatum: 12. April 2021_  \n",
    "_Dozent: Prof. Dr. Dennis Pfisterer_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-sandwich",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-success",
   "metadata": {},
   "source": [
    "## Install Pinot on Kubernetes Cluster\n",
    "\n",
    "To install a simple Pinot setup including 3 servers and a miminal Kafka cluster on a Kubernetes cluster, follow these steps.\n",
    "In our tests we used a single-node cluster on GCP with 4 CPUs and 16 GB of Memory (instance type `n1-standard-4`). We observed that this machine was well utilized during data ingestion, so we recommend using a similarly sized cluster.  \n",
    "See [values.yaml](https://github.com/timebertt/adm-pinot/blob/master/pinot/values.yaml) for further configuration options, e.g. to configure a faster storage class for Pinot's PVCs (we used SSDs (`pd-ssd`) in our tests for better performance).\n",
    "\n",
    "```bash\n",
    "# create and switch to pinot namespace\n",
    "kubectl create namespace pinot\n",
    "kubectl config set-context --current --namespace pinot\n",
    "\n",
    "# install our pre-built helm chart in pinot namespace using the default configuration\n",
    "helm install pinot https://raw.githubusercontent.com/timebertt/adm-pinot/master/assets/pinot-0.1.0.tgz\n",
    "\n",
    "# wait until all pinot components are ready\n",
    "kubectl get pod -w\n",
    "```\n",
    "\n",
    "Once all components are ready, you can open the Pinot dashboard to see if everything is working as expected.\n",
    "```\n",
    "kubectl port-forward svc/pinot-controller 9000 &\n",
    "```\n",
    "Navigate to http://localhost:9000/ to open the Pinot dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-acrylic",
   "metadata": {},
   "source": [
    "## Python requirements\n",
    "\n",
    "Install the Python modules required by this Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -I 'kafka-python==2.0.2' 'names==0.3.0' 'pandas==1.2.3' 'requests==2.25.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "import copy, requests, json, io, re, os, shutil, fileinput, tarfile, time, csv, random, names\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-secondary",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-fraud",
   "metadata": {},
   "source": [
    "Pinot is an open-source distributed highly-available OLAP datastore and built to serve analytical queries on realtime event data. It is developed by engineers of LinkedIn and Uber.\n",
    "LinkedIn is operating Pinot clusters for realtime Online Analytical Processing. They divide their analytics applications into two main categories in their solution landscape: Internal applications and user-facing applications. Internal applications need to process large data volume (trillions of records), but higher query latencies are tolerated. On the opposite, user-facing applications are available for hundreds of millions of LinkedIn members. These applications have a very high query volume and are expected to have a lower latency.\n",
    "Pinot production clusters at LinkedIn are serving tens of thousands queries per second. Overall, more than 50 analytical use cases are supported, and millions of records are ingested per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-brazilian",
   "metadata": {},
   "source": [
    "## Design Principles\n",
    "\n",
    "Key requirements for Pinot include:\n",
    "\n",
    "- high performance (low latency) query execution\n",
    "- near-realtime data ingestion\n",
    "- linear horizontal scalability (in terms of data size, ingestion rate and query rate)\n",
    "- query flexibility to cover a wide range of analytical use cases\n",
    "- high availability of data as well as components (fault tolerance)\n",
    "\n",
    "All of these requirements influence Pinot's fundamental design principles and distributed architecture. We present, how Pinot manages to achieve these goals in the following sections by describing the core concepts and demonstrating the most important mechanisms in Pinot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-chase",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "A Pinot cluster is comprised of multiple distributed components. Each Pinot cluster consists of a controller, one or multiple brokers and multiple servers. Pinot supports multi-tenancy out-of-the-box, as multiple brokers and servers can be assigned to serve specific tenants. A table in pinot consists of columns and rows, which are broken horizontally into shards (named segments).\n",
    "\n",
    "Apache Helix is a generic cluster management framework which is used for automatic management of partitioned and replicated distributed systems by creating and assigning tasks. Apache Zookeeper takes care of coordination and maintenance of the overall cluster state and health. In addition, it stores information about the cluster like server locations of a segment and table schema information. The Controller embeds the Helix agent and is the driver of the cluster. To access CRUD (Create, Read, Update, Delete) Operations on logical storage resources, it provides a REST interface.\n",
    "\n",
    "If a client wants to query data of Pinot tables, the request will be sent to the broker. It routes queries to the appropriate server instances and keeps track on the query routing tables. These routing tables consist of a mapping between segments and server, where the segments reside on. This ensures the right routing of the query to the correct segment. Segments can either consume realtime data or data can be pushed into offline segments. By default, the query load is balanced across all available servers. The broker will return one consolidated result to the client, independent from the fact whether the table is divided into realtime and offline segments.\n",
    "\n",
    "Servers are categorized into offline and realtime servers. According to this categorization, servers in Pinot either host offline or realtime data. The responsibility of a server is defined by the table assignment strategy.\n",
    "\n",
    "If a new realtime table is configured, the realtime server will start consuming data from the streaming source (e.g. Kafka topic). The broker will watch the consumption, detect new segments and maintain them in the query routing list. If a segment has been completed (reached a specific amount of records or was available for a specific timeframe), the controller will upload the segment to the cluster's segment store. The status of the uploaded segment changes from \"consuming\" to \"online\" and the controller will start a new consumption on the realtime server.\n",
    "With batch ingestion, already existing data (e.g. in Hadoop) can be loaded to a Pinot table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-fleet",
   "metadata": {},
   "source": [
    "<img src='https://raw.githubusercontent.com/timebertt/adm-pinot/master/images/Architecture.png' width=\"35%\" height=\"35%\">\n",
    "                                                 \n",
    "Image source: https://docs.pinot.apache.org/basics/architecture (accessed April, 4th 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-devices",
   "metadata": {},
   "source": [
    "In addition to components shown in the above architectural diagram, minions can be deployed to the cluster. They leverage Apache Helix and execute tasks which are provided by the Helix Task Executor Framwork. A minion takes over tasks with intensive workloads from other components like indexing or purging data from a Pinot cluster, for example due to GDPR compliance.\n",
    "The Pinot minion can also be used for Pinot's Offline Flow, which moves records from `REALTIME` tables to corresponding `OFFLINE` tables (covered later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-purchase",
   "metadata": {},
   "source": [
    "## API Interface for Broker and Controller\n",
    "\n",
    "Queries are sent to the broker's REST API (listening on port 8099 by default).\n",
    "To get information about the resources of the Pinot cluster, we are accessing the controller's REST API, which is listening on port 9000.\n",
    "Broker Configurations are defined in a specific `broker.conf` file. The properties define configurations like the query port for the broker or a limit for queries. The latter of which has the purpose to protect brokers and servers against queries returning very large amount of records. A query limit needs to be enabled at cluster level. In our scenario, the parameter `pinot.broker.enable.query.limit.override` is set to false, which means that the broker won't override or add a query limit when the returned record amount is larger than defined in the broker config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\" + \"Broker: \"+ \"\\033[0m\" + json.dumps((requests.get('http://pinot-controller.pinot:9000/v2/brokers/tenants')).json(), indent=2))\n",
    "print(\"\\033[1m\" + \"Health of Controller: \"+ \"\\033[0m\" + requests.get('http://pinot-controller.pinot:9000/pinot-controller/admin').text)\n",
    "print(\"\\033[1m\" + \"Cluster: \"+ \"\\033[0m\" + json.dumps((requests.get('http://pinot-controller.pinot:9000/cluster/configs')).json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-attribute",
   "metadata": {},
   "source": [
    "## Key differences to well-known database technologies\n",
    "\n",
    "In Pinot, data ingestion is append-only. There is no possibility to modify values after ingestion by doing operations like `UPDATE` known from databases like PostgreSQL. Pinot is no replacement for databases in an operational business environment, which usually require updates to data because of the event's nature or due to data correction. For this use cases, Pinot does not fit. Instead, it can enhance use cases requiring fast analytics. However, data can still be purged after ingestion for fullfilling compliance requirements (e.g. GDPR). For this, the Minion can be used to replace entire segments, but in no case, single records can be manipulated.\n",
    "\n",
    "Another difference of Apache Pinot compared to databases like PostgreSQL is that it doesn't support queries requiring movements of large amounts of data between the nodes, like joins. The query engine Presto can be used to join different tables in Pinot, but Presto needs to be set up additionally and is not part of Pinot.\n",
    "\n",
    "Tables in Pinot typically have one primary time column, which is used to manage the time boundary between offline and realtime data in a hybrid table. This may sound familiar to the known concept of time series databases like Influxdb. Both databases are built to handle events with a timestamp, but the timestamp in Pinot is only strictly required for hybrid tables. In addition, Pinot is not only focused on storing timeseries of metrics, it also offers to storm string and bytes values in addition to numeric data types and date time fields. Although Influxdb also support strings to a specific extend, Pinot also offers e.g. text indexing for enhanced full text search.\n",
    "Compared to the timeseries databases like Influxdb, Pinot is optimized for storing time data with a focus on append operations and queries. Update and delete operations on single records are not supported in Apache Pinot, though stream ingestion supports upserts, if a primary key has been defined in the schema.\n",
    "\n",
    "Another key difference of Pinot in comparison to other distributed databases is the heterogeneous nature of its components. Some traditional RDBMSs like for example PostgreSQL can be scaled horizontally to form a cluster by adding more instances, that will each store and manage different partitions (shards) of the dataset. In this case, such a distributed setup is comprised of only a single stateful component, which is started on multiple machines (homogeneous distributed system).\n",
    "In contrast to this, a Pinot cluster is comprised of multiple heterogeneous components (described above), which each serve a specific purpose and are only responsible for a given subtask of the entire system. For example, servers are the stateful components of Pinot, that store and query the actual dataset, while brokers are stateless components, that don't host data themselves and only serve the query frontend for the database. With this, Pinot can be seen as a heterogeneous distributed system, which makes it more complex to deploy and operate, but also serves the key requirements described above (mainly horizontal scalability and fault tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-michael",
   "metadata": {},
   "source": [
    "## Schemas and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-dream",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "To create a table in Pinot, a schema is required. A schema configuration defines fields and data types, this metadata is stored in the Zookeeper.\n",
    "In our examples, we work with data of a fictional online plattform which connects car drivers and passengers to travel together in Germany (ride sharing). \n",
    "\n",
    "Columns in Pinot are of different categories: \n",
    "- dimension columns: support operations like `GROUP BY` and `WHERE` (\"slice and dice\"), e.g. name of the car driver, trip start and end location\n",
    "- metric columns: represent quantitative data and can be used e.g. for aggregation clauses (e.g. payment amount, rating of the driver)\n",
    "- DateTime columns: represent timestamps of records. One DataTime column can be treated as the primary time column, which is defined in the segment config of a table. The primary time column is used for determining boundaries of segments and between offline and realtime data in hybrid tables. A typical operation on DateTime columns is for example `WHERE`, e.g. to select rides of a given day\n",
    "\n",
    "Let's define the example `trips` schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaConfiguration = {\n",
    "  \"schemaName\": \"trips\",\n",
    "  \"dimensionFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"rider_name\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"driver_name\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"license_plate\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"start_location\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"start_zip_code\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "     {\n",
    "      \"name\": \"start_location_state\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    }, \n",
    "    {\n",
    "      \"name\": \"end_location\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"end_zip_code\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "      {\n",
    "      \"name\": \"end_location_state\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    }, \n",
    "    {\n",
    "      \"name\": \"rider_is_premium\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"metricFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"count\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"defaultNullValue\": 1\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"payment_amount\",\n",
    "      \"dataType\": \"FLOAT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"payment_tip_amount\",\n",
    "      \"dataType\": \"FLOAT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"trip_wait_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"rider_rating\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"driver_rating\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"dateTimeFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"trip_start_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"PRIMARY\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"request_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"SECONDARY\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"trip_end_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"SECONDARY\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# create the trips schema\n",
    "response = requests.post('http://pinot-controller.pinot:9000/schemas?override=true', json=schemaConfiguration)\n",
    "print(\"Create Schema: \" + response.text)\n",
    "\n",
    "# list all Schemas\n",
    "response = (requests.get('http://pinot-controller.pinot:9000/schemas')).json()\n",
    "print(\"Get all schemas: \" + str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-recorder",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "Our Pinot tables will consume data from a Kafka Topic in realtime. To be able to consume messages from this topic, data needs to be produced and sent to the topic before.\n",
    "\n",
    "To create and fill our Kafka topic, we first need to create a Kafka producer client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['pinot-kafka.pinot:9092'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-tutorial",
   "metadata": {},
   "source": [
    "The below functions are used to generate random data records for car rides in Germany and inserts them to the Kafka Topic. Each ride consists of driver and passenger details, such as name and rating, measures like payments, details about origin and destination of the trip and different time measures, for example the time stamp when the trip was requested. Date and time of the trip is generated based on the current timestamp (and advancing by roughly 1 second per record)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-semester",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose random city of file containing German cities with postcode\n",
    "if not os.path.exists(\"./pgeocodeDE.txt\"):\n",
    "    # download segment to local file\n",
    "    response = requests.get(\"https://symerio.github.io/postal-codes-data/data/geonames/DE.txt\")\n",
    "    with open(\"./pgeocodeDE.txt\", 'w',encoding='utf8') as out_file:\n",
    "        out_file.write(response.text)\n",
    "    del response\n",
    "\n",
    "geocode_file = open('./pgeocodeDE.txt')\n",
    "geocode_list = list(csv.reader(geocode_file, delimiter='\\t'))[1:] # skip first line (header)\n",
    "random.shuffle(geocode_list)\n",
    "geocode_list = geocode_list[:1000] # take only random 1000 places to generate more overlapping data\n",
    "geocode_file.close()\n",
    "\n",
    "def choose_random_city():\n",
    "    return random.choice(geocode_list)\n",
    "\n",
    "# generate only 1000 driver/rider names to generate more overlapping data\n",
    "names_list = []\n",
    "for i in range(1000):\n",
    "    names_list.append(names.get_full_name())\n",
    "\n",
    "def choose_random_name():\n",
    "    return random.choice(names_list)\n",
    "    \n",
    "# Generation of License Plate\n",
    "# create a pool of letters to choose from\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "numbers = '0123456789'\n",
    "\n",
    "def generate_license_plate():\n",
    "    # generate 3 randomly chosen letters, L1, L2, L3\n",
    "    L1 = random.choice(letters)\n",
    "    L2 = random.choice(letters)\n",
    "    L3 = random.choice(letters)\n",
    "    L4 = random.choice(letters)\n",
    "    # generate 4 randomly chosen numbers, N1, N2, N3, N4\n",
    "    N1 = random.choice(numbers)\n",
    "    N2 = random.choice(numbers)\n",
    "  \n",
    "    # combine it together into one print function\n",
    "    return(L1+L2+'-'+L3+L4+'-'+N1+N2)\n",
    "\n",
    "# Calculation of price based on distance between start city and end destination\n",
    "def calculate_price(v_distance):\n",
    "    v_multiplicator=round(random.uniform(0.8, 2.0),2)\n",
    "    v_price=round(v_distance*v_multiplicator,2)\n",
    "    return(v_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-depression",
   "metadata": {},
   "source": [
    "Let's generate our sample dataset, containing about 300.000 records in total, in order to demonstrate the different Pinot concepts and mechanisms later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin generating trips data at current time\n",
    "start_timestamp_ms = time.time_ns() // 1000000\n",
    "\n",
    "# Generate data\n",
    "num_records = 300000 + random.randint(5000,10000)\n",
    "for i in range(num_records):\n",
    "    v_start_location = choose_random_city()\n",
    "    v_end_location = choose_random_city()\n",
    "    v_distance = random.randint(5,1000)\n",
    "\n",
    "    # add random jitter, in large system our event stream is probably also not strictly sorted\n",
    "    v_requesttime = start_timestamp_ms + i*1000 + random.randint(0,100);\n",
    "\n",
    "    v_waiting_time_millis = random.randint(1,3600000)\n",
    "    v_trip_time = round((v_distance/random.randint(45,60)) * 60 *60*1000)\n",
    "\n",
    "    record = {\n",
    "        \"rider_name\": choose_random_name(),\n",
    "        \"driver_name\": choose_random_name(),\n",
    "        \"license_plate\": generate_license_plate(),\n",
    "        \"start_location\": v_start_location[2],\n",
    "        \"start_zip_code\": v_start_location[1],\n",
    "        \"start_location_state\": v_start_location[3],\n",
    "        \"end_location\": v_end_location[2],\n",
    "        \"end_zip_code\": v_end_location[1],\n",
    "        \"end_location_state\": v_end_location[3],\n",
    "        \"rider_is_premium\": random.randint(0, 1),\n",
    "        \"count\": 1,\n",
    "        \"payment_amount\": calculate_price(v_distance),\n",
    "        \"payment_tip_amount\": random.randint(5,50),\n",
    "        \"trip_wait_time_millis\": v_waiting_time_millis,\n",
    "        \"rider_rating\": random.randint(0,5),\n",
    "        \"driver_rating\": random.randint(0,5),\n",
    "        \"trip_start_time_millis\": v_requesttime + v_waiting_time_millis,\n",
    "        \"request_time_millis\": v_requesttime,\n",
    "        \"trip_end_time_millis\": v_requesttime + v_waiting_time_millis + v_trip_time\n",
    "    }\n",
    " \n",
    "    producer.send('trips', value=record)\n",
    "        \n",
    "    if i % 5000 == 0:\n",
    "        print(f'{i} records generated')\n",
    "\n",
    "print(f'done generating {num_records} records, ready to do some fancy analytics!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-agriculture",
   "metadata": {},
   "source": [
    "### Tables\n",
    "\n",
    "Tables represent a collection of related data in Pinot. A table either have the type `OFFLINE` (ingesting pre-built pinot-segments from external stores) or `REALTIME` (data ingestion from streams). The user is not required to know the type of a table when querying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def query_sql(query):\n",
    "    print(\"query: \" + query)\n",
    "    return requests.get('http://pinot-broker.pinot:8099/query/sql', params={\n",
    "        \"sql\" : query,\n",
    "        \"trace\": \"true\"\n",
    "    }).json()\n",
    "\n",
    "def query_result_to_dataframe(result):\n",
    "    return pd.DataFrame(columns=result['resultTable']['dataSchema']['columnNames'], data=result['resultTable']['rows'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-clause",
   "metadata": {},
   "source": [
    "To configure a table, properties like name, type and indexing are required. In the following example, we create an example table which is consuming data from the Kafka topic filled above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_config = {\n",
    "  \"tableName\": \"trips\",\n",
    "  \"tableType\": \"REALTIME\",\n",
    "  \"segmentsConfig\": {\n",
    "    \"timeColumnName\": \"trip_start_time_millis\",\n",
    "    \"timeType\": \"MILLISECONDS\",\n",
    "    \"retentionTimeUnit\": \"DAYS\",\n",
    "    \"retentionTimeValue\": \"60\",\n",
    "    \"schemaName\": \"trips\",\n",
    "    \"replication\": \"1\",\n",
    "    \"replicasPerPartition\": \"1\"\n",
    "  },\n",
    "  \"tenants\": {},\n",
    "  \"tableIndexConfig\": {\n",
    "    \"loadMode\": \"MMAP\",\n",
    "    \"streamConfigs\": {\n",
    "      \"streamType\": \"kafka\",\n",
    "      \"stream.kafka.consumer.type\": \"simple\",\n",
    "      \"stream.kafka.topic.name\": \"trips\",\n",
    "      \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n",
    "      \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n",
    "      \"stream.kafka.zk.broker.url\": \"pinot-kafka-zookeeper:2181\",\n",
    "      \"stream.kafka.broker.list\": \"pinot-kafka:9092\",\n",
    "      \"realtime.segment.flush.threshold.time\": \"12h\",\n",
    "      \"realtime.segment.flush.threshold.size\": \"20000\",\n",
    "      \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\"\n",
    "    },\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"customConfigs\": {}\n",
    "  }\n",
    "} \n",
    "\n",
    "response = requests.post('http://pinot-controller.pinot:9000/tables', json=table_config)\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-harrison",
   "metadata": {},
   "source": [
    "After creation, data records of the Kafka Topic are loaded into the table. To execute a query, the SQL statement is sent to the broker of the Pinot cluster. The response contains the result records, as well as query statistics of the execution.\n",
    "\n",
    "While our data is loading, let's query the example table to find out the top 5 states where trips of our ride sharing platform start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result_to_dataframe(query_sql(\"\"\"\n",
    "    SELECT start_location_state, SUM(count) as trips_count\n",
    "    FROM trips\n",
    "    GROUP BY start_location_state\n",
    "    ORDER BY trips_count DESC\n",
    "    LIMIT 5\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-desire",
   "metadata": {},
   "source": [
    "# Segmentation in Pinot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-retention",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Table contents in Pinot are expected to grow infinitely and thus need to be distributed across multiple nodes. Therefore, the tables' dataset is split into segments, which are comparable to shards/partitions in classical RDBMSs. In Pinot, segmentation is done in a time-based fashion, meaning that configured timestamps of records in a given segment will be close to each other.\n",
    "Segments store all columns of a table and organize data in columnar orientation for high encoding efficiency and optional pre-aggregation of metrics. In addition to the data itself, segments contain indices and other lookup-related data structures like dictionaries.\n",
    "\n",
    "As Pinot is not a general-purpose database (data is immutable), it cannot be used as an application's \"main datastore\". Like other OLAP stores, Pinot is supposed to run next to the application's \"main datastore\" and its data has to be imported separately (ingestion). In order to facilitate near-realtime analytical queries, for example like the ones powering LinkedIn's well-known \"Who viewed my profile\" functionality, data is typically ingested into Pinot via event streaming platforms, like Apache Kafka (stream ingestion). In contrast to classical RDBMSs, Pinot comes with built-in support for directly reading from Kafka event streams.\n",
    "However, data can also be ingested from traditional batch processing workflows, for example realized with Apache Hadoop or Apache Spark (batch ingestion).\n",
    "\n",
    "Pinot tables are either defined as realtime or offline tables. Tables of both types are broken into segments. For realtime tables, data is consumed directly from event streams by Pinot servers as-is without any additional processing. Segments are built inside Pinot and are completed once a given threshold in size or time is reached. Segments for offline tables are built outside of Pinot in batch processing jobs, that might perform additional data deduplication or similar processing, and uploaded to the Pinot controller. Both table types might be combined to form hybrid tables, that allow both realtime analytics as well as long-term data storage (covered later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-chair",
   "metadata": {},
   "source": [
    "## Realtime Data Ingestion\n",
    "\n",
    "To demonstrate how segments work in Pinot, we're going to focus on realtime data ingestion first. In the following examples, we'll be using the controller's and broker's REST APIs in order to dynamically create realtime tables, retrieve segment metadata and execute SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpers for the upcoming examples\n",
    "def server_name_from_instance(instance):\n",
    "    return re.search('pinot-server-[0-9]+', instance).group()\n",
    "\n",
    "def extract_query_statistics_from_result(result):\n",
    "    query_statistics_fields = [\"numServersQueried\",\"numServersResponded\",\"numSegmentsQueried\",\"numSegmentsProcessed\",\"numSegmentsMatched\",\"numConsumingSegmentsQueried\",\"numDocsScanned\",\"numEntriesScannedInFilter\",\"numEntriesScannedPostFilter\",\"numGroupsLimitReached\",\"totalDocs\",\"timeUsedMs\"]\n",
    "    return { key: result[key] for key in query_statistics_fields }\n",
    "\n",
    "def extract_query_statistics_from_result_dataframe(result):\n",
    "    return pd.DataFrame({\"value\": extract_query_statistics_from_result(result)})\n",
    "\n",
    "ordinal_pattern = re.compile(r'__[0-9]+__([0-9]+)__')\n",
    "def sort_by_ascending_ordinal(segments):\n",
    "    segments.sort(key=lambda L: (int(ordinal_pattern.search(L).group(1)), L))\n",
    "\n",
    "def segment_metadata_for_table(table):\n",
    "    segments = requests.get(f'http://pinot-controller.pinot:9000/segments/{table}').json()\n",
    "    \n",
    "    segment_metadata = {}\n",
    "    for segments_item in segments:\n",
    "        for table_type, type_segments in segments_item.items():\n",
    "            for segment in type_segments:\n",
    "                segment_type_name = f\"{segment}_{table_type}\"\n",
    "                segment_metadata[segment_type_name] = requests.get(f'http://pinot-controller.pinot:9000/segments/{table}/{segment}/metadata').json()\n",
    "    \n",
    "    return segment_metadata\n",
    "\n",
    "def segment_metadata_of_nth_segment(segment_metadata, n, table_type=\"REALTIME\"):\n",
    "    segments_of_type = []\n",
    "    for segment in segment_metadata.keys():\n",
    "        if segment.endswith(\"_\" + table_type):\n",
    "            segments_of_type.append(segment)\n",
    "    \n",
    "    sort_by_ascending_ordinal(segments_of_type)\n",
    "    return segment_metadata[segments_of_type[n]]\n",
    "\n",
    "\n",
    "def start_time_of_nth_segment(segment_metadata, n, table_type=\"REALTIME\"):\n",
    "    return segment_metadata_of_nth_segment(segment_metadata, n, table_type)[\"segment.start.time\"]\n",
    "\n",
    "def wait_for_table_to_finish_loading(table, wait_time=15):\n",
    "    last_total_docs = -1\n",
    "    while True:\n",
    "        response = requests.post('http://pinot-broker.pinot:8099/query/sql', json={\"sql\" : f\"SELECT * FROM {table} LIMIT 1\"}).json()\n",
    "        total_docs = response[\"totalDocs\"]\n",
    "        if total_docs == last_total_docs:\n",
    "            print(f\"--Consumption of generated data for table {table} finished, (loaded {last_total_docs} docs)--\")\n",
    "            break\n",
    "        \n",
    "        last_total_docs = total_docs\n",
    "        print(f\"waiting for table {table} to finish loading (loaded {last_total_docs} docs)\")\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-neighbor",
   "metadata": {},
   "source": [
    "At first, we will create two realtime tables. Both will be using the `trips` schema created above and read from the `trips` topic in Kafka, that was also created and filled with random records above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-pearl",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# common configuration used for both tables\n",
    "table_config_template = {\n",
    "  \"tableName\": \"\",\n",
    "  \"tableType\": \"REALTIME\",\n",
    "  \"segmentsConfig\": {\n",
    "    \"timeColumnName\": \"trip_start_time_millis\",\n",
    "    \"timeType\": \"MILLISECONDS\",\n",
    "    \"retentionTimeUnit\": \"DAYS\",\n",
    "    \"retentionTimeValue\": \"60\",\n",
    "    \"schemaName\": \"trips\",\n",
    "  },\n",
    "  \"tenants\": {},\n",
    "  \"tableIndexConfig\": {\n",
    "    \"loadMode\": \"MMAP\",\n",
    "    \"invertedIndexColumns\": [\n",
    "        \"rider_name\",\n",
    "        \"driver_name\",\n",
    "        \"start_location\",\n",
    "        \"end_location\"\n",
    "    ],\n",
    "    \"streamConfigs\": {\n",
    "      \"streamType\": \"kafka\",\n",
    "      \"stream.kafka.topic.name\": \"trips\",\n",
    "      \"stream.kafka.consumer.type\": \"simple\",\n",
    "      \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n",
    "      \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n",
    "      \"stream.kafka.zk.broker.url\": \"pinot-kafka-zookeeper:2181\",\n",
    "      \"stream.kafka.broker.list\": \"pinot-kafka:9092\",\n",
    "      \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\"\n",
    "    }\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"customConfigs\": {}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-notion",
   "metadata": {},
   "source": [
    "Pinot servers will continuously read from the Kafka topic into memory and compile a segment until a configured threshold is reached. The first table is configured to flush the new in-memory segment to disk, once either 12 hours have passed or the segment contains 80,000 rows (which will be the case for our example, as the data is already waiting in the Kafka stream)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create first table\n",
    "table_config = copy.deepcopy(table_config_template)\n",
    "table_config[\"tableName\"] = \"trips_segmentation_1\"\n",
    "table_config[\"segmentsConfig\"][\"replication\"] = \"1\"\n",
    "table_config[\"segmentsConfig\"][\"replicasPerPartition\"] = \"1\"\n",
    "table_config[\"tableIndexConfig\"][\"streamConfigs\"][\"realtime.segment.flush.threshold.time\"] = \"12h\"\n",
    "table_config[\"tableIndexConfig\"][\"streamConfigs\"][\"realtime.segment.flush.threshold.size\"] = \"80000\"\n",
    "display(requests.post('http://pinot-controller.pinot:9000/tables', json=table_config).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-remedy",
   "metadata": {},
   "source": [
    "In contrast to the first table, the second one will target a segment size of 50,000 rows and will additionally create 3 replicas of each segment on different server instances for data availability (fault tolerance) and load distribution of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create second table\n",
    "table_config = copy.deepcopy(table_config_template)\n",
    "table_config[\"tableName\"] = \"trips_segmentation_2\"\n",
    "table_config[\"segmentsConfig\"][\"replication\"] = \"3\"\n",
    "table_config[\"segmentsConfig\"][\"replicasPerPartition\"] = \"3\"\n",
    "table_config[\"tableIndexConfig\"][\"streamConfigs\"][\"realtime.segment.flush.threshold.time\"] = \"12h\"\n",
    "table_config[\"tableIndexConfig\"][\"streamConfigs\"][\"realtime.segment.flush.threshold.size\"] = \"50000\"\n",
    "display(requests.post('http://pinot-controller.pinot:9000/tables', json=table_config).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-teaching",
   "metadata": {},
   "source": [
    "Let's wait for the tables to finish loading the data from Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_table_to_finish_loading(\"trips_segmentation_1\")\n",
    "wait_for_table_to_finish_loading(\"trips_segmentation_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-pleasure",
   "metadata": {},
   "source": [
    "The controller stores metadata for each segment, which can be viewed via its REST API. Each segment's metadata contains general information such as the table type, table name and time unit as well as segment-specific information such as the number of records (`segment.total.docs`), the timestamp of the segment's first and last record (`segment.start.time`, `segment.end.time`) and the segment's status (`segment.realtime.status`).\n",
    "New realtime segments start in status `IN_PROGRESS`, which means that the segment is currently consuming data from the Kafka topic. Once the size or time threshold is reached, the consuming servers start a segment commit protocol in order to agree on the last record that shall be included in the segment. Once the commit protocol is completed, the segment transitions to `DONE` and the servers flush the data to disk. Afterwards, a new segment is started again to consume further data from the event stream.\n",
    "\n",
    "We can now query the controller's REST API to retrieve metadata for all segments in both our tables.\n",
    "The first table contains less segments, but each segment contains a higher number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_metadata_1 = segment_metadata_for_table(\"trips_segmentation_1\")\n",
    "pd.DataFrame(segment_metadata_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-liability",
   "metadata": {},
   "source": [
    "The segment metadata for the second table shows more segments. Each of them has a lower number of total records and 3 replicas (`segment.realtime.numReplicas`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_metadata_2 = segment_metadata_for_table(\"trips_segmentation_2\")\n",
    "pd.DataFrame(segment_metadata_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-screw",
   "metadata": {},
   "source": [
    "Pinot brokers are responsible for executing queries against the database. When a broker receives a new query, it sends multiple subqueries to Pinot servers that are hosting the segments belonging to the queried table. Once it has received results from all queried servers, it merges the subresults and returns the aggregated result to the client.\n",
    "In order to efficiently execute queries, brokers use segment metadata to figure out, which segments need to be queried. For example, if we want to list the top 5 drivers in terms of trips count in a given timeframe, only the segments hosting data of the timeframe need to be queried.\n",
    "\n",
    "To demonstrate this behaviour, we call the broker's REST API and query data from the time range of the first segment (before start time of the second segment). In the returned query statistics we can see, that not all segments of the table (`numSegmentsQueried`) are actually processed, but only 2 of them (`numSegmentsMatched`). This is because the last (the consuming) segment is always queried, as the metadata is not yet completed and so the broker can't tell upfront, if the last segment might contain relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from first segment (consuming segment is always queried because of uncompleted metadata)\n",
    "query_for_trips_segmentation_1 = f\"\"\"\n",
    "    SELECT driver_name, sum(count) AS trips_count\n",
    "    FROM trips_segmentation_1\n",
    "    WHERE trip_start_time_millis BETWEEN {start_time_of_nth_segment(segment_metadata_1, 0)} AND {int(start_time_of_nth_segment(segment_metadata_1, 1))-1}\n",
    "    GROUP BY driver_name\n",
    "    ORDER BY trips_count desc\n",
    "    LIMIT 5\"\"\"\n",
    "\n",
    "query_result = query_sql(query_for_trips_segmentation_1)\n",
    "display(query_result_to_dataframe(query_result))\n",
    "display(extract_query_statistics_from_result_dataframe(query_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-brooklyn",
   "metadata": {},
   "source": [
    "The second query targets the second table and lists the top 5 drivers according to rating over the time range of the first 3 segments.\n",
    "Similarly to the query above, only relevant segments need to be processed for this query.\n",
    "However, in contrast to the first query execution, the broker can make use of the segment replication and can distribute the subqueries for individual segments across different servers (note that `numServersQueried` is now 3 instead of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from first 3 segments (consuming segment is always queried because of uncompleted metadata)\n",
    "query_for_trips_segmentation_2 = f\"\"\"\n",
    "    SELECT driver_name, avg(driver_rating) AS rating\n",
    "    FROM trips_segmentation_2\n",
    "    WHERE trip_start_time_millis BETWEEN {start_time_of_nth_segment(segment_metadata_2, 0)} AND {int(start_time_of_nth_segment(segment_metadata_2, 3))-1}\n",
    "    GROUP BY driver_name\n",
    "    ORDER BY rating desc\n",
    "    LIMIT 5\"\"\"\n",
    "\n",
    "query_result = query_sql(query_for_trips_segmentation_2)\n",
    "display(query_result_to_dataframe(query_result))\n",
    "display(extract_query_statistics_from_result_dataframe(query_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-privilege",
   "metadata": {},
   "source": [
    "## Query Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-reasoning",
   "metadata": {},
   "source": [
    "In order to efficiently distribute queries across the fleet of servers, brokers maintain so called routing tables, which contain mappings between segments of a table and servers where they are hosted on. \n",
    "In case of replicated segments (like in the second table), the routing table contains entries for all servers hosting a single segment. When queries arrive at the broker, the routing tables and segment metadata allow to efficiently scatter queries across servers to balance load across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpers for the upcoming examples\n",
    "def routing_table_for_query(query):\n",
    "    print(\"query: \" + query)\n",
    "    return requests.get('http://pinot-broker.pinot:8099/debug/routingTable/sql', params={\n",
    "        \"query\" : query\n",
    "    }).json()\n",
    "\n",
    "def routing_table_for_table(table):\n",
    "    return requests.get(f'http://pinot-broker.pinot:8099/debug/routingTable/{table}').json()\n",
    "\n",
    "def external_view_for_table(table):\n",
    "    return requests.get(f'http://pinot-controller.pinot:9000/tables/{table}/externalview').json()\n",
    "\n",
    "def routing_table_for_query_dataframe(query):\n",
    "    rt = routing_table_for_query(query)\n",
    "    rt_data = {}\n",
    "\n",
    "    for server, server_segments in rt.items():\n",
    "        server_name = server_name_from_instance(server)\n",
    "        for s in server_segments:\n",
    "            rt_data[s] = server_name\n",
    "\n",
    "    rt_data_list = []\n",
    "    for segment, server in rt_data.items():\n",
    "        rt_data_list.append({\"segment\": segment, \"server\": server})\n",
    "\n",
    "    rt_data_list.sort(key=lambda L: (int(ordinal_pattern.search(L[\"segment\"]).group(1)), L))\n",
    "    return pd.DataFrame(rt_data_list)\n",
    "\n",
    "def routing_table_for_table_dataframe(table):\n",
    "    rt = routing_table_for_table(table)\n",
    "    rt_data = {}\n",
    "\n",
    "    for table_name_type, table_rt in rt.items():\n",
    "        table_type = re.search('REALTIME|OFFLINE', table_name_type).group()\n",
    "        for server, server_segments in table_rt.items():\n",
    "            server_name = server_name_from_instance(server)\n",
    "            for s in server_segments:\n",
    "                try:\n",
    "                    rt_data[s][table_type] = server_name\n",
    "                except KeyError:\n",
    "                    rt_data[s] = {table_type: server_name}\n",
    "\n",
    "    rt_data_list = []\n",
    "    for segment, type_server in rt_data.items():\n",
    "        segment_data = {\"segment\": segment}\n",
    "        for table_type, server in type_server.items():\n",
    "            segment_data[table_type] = server\n",
    "        rt_data_list.append(segment_data)\n",
    "\n",
    "    rt_data_list.sort(key=lambda L: (int(ordinal_pattern.search(L[\"segment\"]).group(1)), L))\n",
    "    return pd.DataFrame(rt_data_list)\n",
    "\n",
    "def external_view_for_table_dataframe(table):\n",
    "    ev = external_view_for_table(table)\n",
    "    ev_data = {}\n",
    "\n",
    "    for table_type, ev_per_type in ev.items():\n",
    "        if ev_per_type == None:\n",
    "            continue\n",
    "        \n",
    "        for segment, segment_servers in ev_per_type.items():\n",
    "            if not segment in ev_data:\n",
    "                ev_data[segment] = {}\n",
    "            for server, state in segment_servers.items():\n",
    "                server_name = server_name_from_instance(server)\n",
    "                try:\n",
    "                    ev_data[segment][table_type].append(server_name)\n",
    "                except KeyError:\n",
    "                    ev_data[segment][table_type] = [server_name]\n",
    "\n",
    "    return pd.DataFrame(ev_data).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-burke",
   "metadata": {},
   "source": [
    "First, let's take a look at the external view for both tables. The external view shows an overview, which segments are available on which server. In case of the first table, each segment is only available on a single server. The second table has a replica of each segment on every server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(external_view_for_table_dataframe(\"trips_segmentation_1\"))\n",
    "display(external_view_for_table_dataframe(\"trips_segmentation_2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-vietnam",
   "metadata": {},
   "source": [
    "We can use the broker's debug endpoint to retrieve a routing table for a specific SQL query. This can be seen as a query execution plan for segments distributed across multiple servers. Similar to calculating an efficient query execution plan in classical RDBMSs, Pinot takes a look at metadata, statistics and server associations.\n",
    "The routing table might change everytime an identical query is executed, as brokers try to distribute compute load across servers hosting the same segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-lying",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_table_for_query_dataframe(query_for_trips_segmentation_1.replace(\"trips_segmentation_1\", \"trips_segmentation_1_REALTIME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-wagner",
   "metadata": {},
   "source": [
    "For the second query, the routing table shows, that the broker will try to equally distribute load between all the servers, as the segments are replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_table_for_query_dataframe(query_for_trips_segmentation_2.replace(\"trips_segmentation_2\", \"trips_segmentation_2_REALTIME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-performer",
   "metadata": {},
   "source": [
    "## Advanced Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-tuner",
   "metadata": {},
   "source": [
    "The presented tables are rather simple and just demonstrate the basic mechanisms of segmentation, replication and query routing in Pinot. However, Pinot offers much more advanced configuration options for tweaking segment replication, availability and placement in large-scale Pinot clusters.\n",
    "\n",
    "For example, Pinot servers can be grouped in so called \"replica groups\", that can be spread across different availability zones. Segment replicas will then be assigned to servers in different replica groups in order to achieve high-availability setups. Furthermore, segments can be partitioned based on column values to further increase query performance by decreasing the number of segments that need to be processed for a given query. This is very similar to partitioning/sharding in typical RDBMSs.\n",
    "Additionally, servers can be assigned to different tenants for sharing a cluster across teams or grouped into server-pools to achieve no-downtime rolling restarts of large clusters.\n",
    "\n",
    "All of these options show, that segmentation in Pinot is in the simplest aspects quite comparable to sharding mechanism in other database systems, but it is also much more advanced to support large-scale analytical use-cases while maintaining high performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-thunder",
   "metadata": {},
   "source": [
    "# Batch Ingestion and Hybrid Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-cemetery",
   "metadata": {},
   "source": [
    "As mentioned earlier, Pinot also support ingesting data from batch processing jobs. For offline tables, the same principles apply as for realtime tables with regards to segmentation and query routing. \n",
    "Though, segments are compiled and packaged outside Pinot. For this purpose, Pinot offers different mechanisms to load pre-built segments from object stores (such as S3) or HDFS or to build new segments using Hadoop and/or Spark.\n",
    "Segments are packaged as gzipped tar-archives (including data, index maps, column statistics) and can be uploaded to and downloaded from the controller.\n",
    "\n",
    "While offline tables can be used standalone similar to the realtime tables presented above, a more interesting option is to combine an offline and a realtime table to form a hybrid table.\n",
    "Hybrid tables are comprised of two individual tables, one offline table and one hybrid table, both sharing the same name, schema and ‚Äì most importantly ‚Äì time column. The hybrid table can be queried just like any other table, but the broker will transparently rewrite queries to fetch older records from the offline table and newer records from the realtime table.\n",
    "This allows to process, deduplicate and sanitize records before pushing them to long-term storage. This is a key differentiator between Pinot and other databases and OLAP stores. It allows Pinot to achieve high-throughput ingestion, low-latency realtime analytics, while still allowing to backfill data in batch processing.\n",
    "\n",
    "Since version `0.6.0` Pinot also offers a mechanism to regularly move records from a realtime table to the corresponding offline table. To configure this, the user can schedule a task, which should be executed on a minion instance for example once every day. The task execution will then take over downloading, transforming, aggregating, sorting and uploading of segments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-termination",
   "metadata": {},
   "source": [
    "To demonstrate how batch ingestion and hybrid tables work in Pinot without setting up an external batch processing system or periodic segment transformation job, we're going to create a realtime table reading from our Kafka `trips` topic, download completed segments from the controller and re-upload them as offline segments.\n",
    "\n",
    "First, we need to create both tables (note the shared name and schema):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common configuration used for both tables types\n",
    "table_config_template = {\n",
    "  \"tableName\": \"trips_hybrid\",\n",
    "  \"segmentsConfig\": {\n",
    "    \"timeColumnName\": \"trip_start_time_millis\",\n",
    "    \"timeType\": \"MILLISECONDS\",\n",
    "    \"retentionTimeUnit\": \"DAYS\",\n",
    "    \"retentionTimeValue\": \"60\",\n",
    "    \"schemaName\": \"trips\",\n",
    "    \"replication\": \"1\"\n",
    "  },\n",
    "  \"tenants\": {},\n",
    "  \"tableIndexConfig\": {\n",
    "    \"loadMode\": \"MMAP\",\n",
    "    \"invertedIndexColumns\": [\n",
    "        \"rider_name\",\n",
    "        \"driver_name\",\n",
    "        \"start_location\",\n",
    "        \"end_location\"\n",
    "    ]\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"customConfigs\": {}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create offline table\n",
    "table_config = copy.deepcopy(table_config_template)\n",
    "table_config[\"tableType\"] = \"OFFLINE\"\n",
    "print(requests.post('http://pinot-controller.pinot:9000/tables', json=table_config).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create realtime table\n",
    "table_config = copy.deepcopy(table_config_template)\n",
    "table_config[\"tableType\"] = \"REALTIME\"\n",
    "table_config[\"segmentsConfig\"][\"replicasPerPartition\"] = \"1\"\n",
    "table_config[\"tableIndexConfig\"][\"streamConfigs\"] = {\n",
    "  \"streamType\": \"kafka\",\n",
    "  \"stream.kafka.consumer.type\": \"simple\",\n",
    "  \"stream.kafka.topic.name\": \"trips\",\n",
    "  \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n",
    "  \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n",
    "  \"stream.kafka.zk.broker.url\": \"pinot-kafka-zookeeper:2181\",\n",
    "  \"stream.kafka.broker.list\": \"pinot-kafka:9092\",\n",
    "  \"realtime.segment.flush.threshold.time\": \"12h\",\n",
    "  \"realtime.segment.flush.threshold.size\": \"50000\",\n",
    "  \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\"\n",
    "}\n",
    "print(requests.post('http://pinot-controller.pinot:9000/tables', json=table_config).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-ability",
   "metadata": {},
   "source": [
    "Let's again wait for our table to finish loading the data from Kafka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_table_to_finish_loading(\"trips_hybrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-algeria",
   "metadata": {},
   "source": [
    "Let's take a look at the external view of the hybrid table before touching it. We can see some realtime segments, that were built from the data stream from Kafka, but there are no offline segments so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_view_for_table_dataframe(\"trips_hybrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for transforming realtime segments to offline segment\n",
    "tmp_hybrid_basedir = \"/tmp/trips_hybrid\"\n",
    "# cleanup old artifacts if any\n",
    "shutil.rmtree(tmp_hybrid_basedir, ignore_errors=True)\n",
    "os.mkdir(tmp_hybrid_basedir)\n",
    "\n",
    "def path_for_realtime_tar(segment_name):\n",
    "    return f\"{tmp_hybrid_basedir}/{segment_name}.tar.gz\"\n",
    "\n",
    "def path_for_offline_dir(segment_name):\n",
    "    return f\"{tmp_hybrid_basedir}/{segment_name}_offline\"\n",
    "\n",
    "def path_for_offline_tar(segment_name):\n",
    "    return f\"{tmp_hybrid_basedir}/{segment_name}_offline.tar.gz\"\n",
    "\n",
    "def download_segment(segment_metadata):\n",
    "    segment_name = segment_metadata[\"segment.name\"]\n",
    "    download_url = segment_metadata[\"segment.realtime.download.url\"]\n",
    "    segment_realtime_tar = path_for_realtime_tar(segment_name)\n",
    "\n",
    "    # cleanup old downloads\n",
    "    try:\n",
    "        os.remove(segment_realtime_tar)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # download realtime segment tar\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    with open(segment_realtime_tar, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response.raw, out_file)\n",
    "    del response\n",
    "    \n",
    "    print(f\"segment {segment_name} downloaded from {download_url} to {segment_realtime_tar}\")\n",
    "    return segment_realtime_tar\n",
    "\n",
    "def untar_segment(segment_metadata):\n",
    "    segment_name = segment_metadata[\"segment.name\"]\n",
    "    segment_offline_basedir = path_for_offline_dir(segment_name)\n",
    "    segment_realtime_tar = path_for_realtime_tar(segment_name)\n",
    "\n",
    "    # cleanup old artifacts if any\n",
    "    shutil.rmtree(segment_offline_basedir, ignore_errors=True)\n",
    "\n",
    "    # extract downloaded segment tar\n",
    "    with tarfile.open(segment_realtime_tar, 'r:gz') as tar:\n",
    "        tar.extractall(path=segment_offline_basedir)\n",
    "\n",
    "    print(f\"segment {segment_name} untarred to {segment_offline_basedir}\")\n",
    "    return segment_offline_basedir\n",
    "\n",
    "def transform_segment(segment_metadata):\n",
    "    realtime_table_name = segment_metadata[\"segment.table.name\"]\n",
    "    offline_table_name = realtime_table_name.replace(\"REALTIME\", \"OFFLINE\")\n",
    "    segment_name = segment_metadata[\"segment.name\"]\n",
    "    segment_offline_basedir = path_for_offline_dir(segment_name)\n",
    "    \n",
    "    # modify metadata.properties of segment\n",
    "    segment_offline_dir = segment_offline_basedir + \"/\" + segment_name\n",
    "    metadata_file = segment_offline_dir + \"/v3/metadata.properties\"\n",
    "    metadata_contents = None\n",
    "    with open(metadata_file, 'r') as file:\n",
    "      metadata_contents = file.read()\n",
    "    \n",
    "    metadata_contents = metadata_contents.replace(realtime_table_name, offline_table_name)\n",
    "    \n",
    "    with open(metadata_file, 'w') as file:\n",
    "      file.write(metadata_contents)\n",
    "    del metadata_contents\n",
    "\n",
    "    # create new offline segment tar\n",
    "    segment_offline_tar = path_for_offline_tar(segment_name)\n",
    "    with tarfile.open(segment_offline_tar, 'w:gz') as tar:\n",
    "        tar.add(segment_offline_dir, arcname=segment_name)\n",
    "\n",
    "    print(f\"segment {segment_name} transformed to offline segment to {segment_offline_tar}\")\n",
    "    return segment_offline_tar\n",
    "\n",
    "def upload_segment_to_offline_table(segment_metadata):\n",
    "    realtime_table_name = segment_metadata[\"segment.table.name\"]\n",
    "    segment_name = segment_metadata[\"segment.name\"]\n",
    "    segment_offline_tar = path_for_offline_tar(segment_name)\n",
    "    table_name = realtime_table_name.replace(\"_REALTIME\", \"_OFFLINE\")\n",
    "    \n",
    "    # POST segment as multipart/form-data for key 'segment'\n",
    "    with open(segment_offline_tar, 'rb') as tar:\n",
    "        response = requests.post(f'http://pinot-controller.pinot:9000/v2/segments?table={table_name}', files={\n",
    "            'segment': tar\n",
    "        })\n",
    "        print(response)\n",
    "        print(response.json())\n",
    "\n",
    "def transform_and_upload_nth_segment_to_offline_table(segment_metadata, n):\n",
    "    nth_meta = segment_metadata_of_nth_segment(segment_metadata, n, table_type=\"REALTIME\")\n",
    "    \n",
    "    # download, transform and upload all in one row\n",
    "    download_segment(nth_meta)\n",
    "    untar_segment(nth_meta)\n",
    "    transform_segment(nth_meta)\n",
    "    upload_segment_to_offline_table(nth_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-recording",
   "metadata": {},
   "source": [
    "Now, we fetch the first two segments from the controller, manipulate the metadata and re-upload them to the controller as offline segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_metadata_hybrid = segment_metadata_for_table(\"trips_hybrid\")\n",
    "\n",
    "transform_and_upload_nth_segment_to_offline_table(segment_metadata_hybrid, 0)\n",
    "transform_and_upload_nth_segment_to_offline_table(segment_metadata_hybrid, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-alexander",
   "metadata": {},
   "source": [
    "The external view for our hybrid table now shows the newly added offline segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_view_for_table_dataframe(\"trips_hybrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-watts",
   "metadata": {},
   "source": [
    "This example query lists the top 5 riders in terms of total trip time. It shows that hybrid tables can be queried in the exact same way, as realtime tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_for_hybrid = \"\"\"\n",
    "    SELECT rider_name, sum(trip_end_time_millis - trip_start_time_millis) / (60*60*1000) AS trip_time_sum\n",
    "    FROM trips_hybrid\n",
    "    GROUP BY rider_name\n",
    "    ORDER BY trip_time_sum DESC\n",
    "    LIMIT 5\n",
    "    \"\"\"\n",
    "\n",
    "query_result = query_sql(query_for_hybrid)\n",
    "query_result_to_dataframe(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-asian",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-luxury",
   "metadata": {},
   "source": [
    "## Table Creation with Different Indexing Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-algeria",
   "metadata": {},
   "source": [
    "To demonstrate the different indexing options and mechanisms, that Pinot offers, we will create some tables with different index configurations, describe their key properties and compare query performance.\n",
    "The index configurations are applied to the `tableIndexConfig`-section the table configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to collect details of created tables\n",
    "table_list = []\n",
    "\n",
    "table_config_template = {\n",
    "  \"tableType\": \"REALTIME\",\n",
    "  \"segmentsConfig\": {\n",
    "    \"timeColumnName\": \"trip_start_time_millis\",\n",
    "    \"timeType\": \"MILLISECONDS\",\n",
    "    \"retentionTimeUnit\": \"DAYS\",\n",
    "    \"retentionTimeValue\": \"60\",\n",
    "    \"schemaName\": \"trips\",\n",
    "    \"replication\": \"1\",\n",
    "    \"replicasPerPartition\": \"1\"\n",
    "  },\n",
    "  \"tenants\": {},\n",
    "  \"tableIndexConfig\": {\n",
    "    \"loadMode\": \"MMAP\",\n",
    "    \"streamConfigs\": {\n",
    "      \"streamType\": \"kafka\",\n",
    "      \"stream.kafka.consumer.type\": \"simple\",\n",
    "      \"stream.kafka.topic.name\": \"trips\",\n",
    "      \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n",
    "      \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n",
    "      \"stream.kafka.zk.broker.url\": \"pinot-kafka-zookeeper:2181\",\n",
    "      \"stream.kafka.broker.list\": \"pinot-kafka:9092\",\n",
    "      \"realtime.segment.flush.threshold.time\": \"12h\",\n",
    "      \"realtime.segment.flush.threshold.size\": \"20000\",\n",
    "      \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\"\n",
    "    }\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"customConfigs\": {}\n",
    "  }\n",
    "} \n",
    "\n",
    "# helper function\n",
    "def createTable(newTable_name, index_text, tableconfig_json):\n",
    "    # Input: Name of new table, index description, table configuration in json structure\n",
    "    response = requests.post('http://pinot-controller.pinot:9000/tables', json=tableconfig_json)\n",
    "    print(response)\n",
    "    print(response.text)\n",
    "    table_list.append([newTable_name, index_text])\n",
    "\n",
    "\n",
    "execution_start_time = int(round(time.time() * 1000))\n",
    "\n",
    "# Create a new table with default index for each column (no configuration required)\n",
    "newTable_defaultIndex = copy.deepcopy(table_config_template)\n",
    "newTable_defaultIndex[\"tableName\"] = \"trips_default_index\"\n",
    "createTable(newTable_defaultIndex[\"tableName\"], 'Default Index (Dictionary-encoded forward index with bit compression) for each column', newTable_defaultIndex)\n",
    "\n",
    "# Create a new table with raw value forward index\n",
    "newTable_rawForwardIndex = copy.deepcopy(table_config_template)\n",
    "newTable_rawForwardIndex[\"tableName\"] = \"trips_rawForwardIndex\"\n",
    "newTable_rawForwardIndex[\"tableIndexConfig\"][\"noDictionaryColumns\"] = [\"start_location\"]\n",
    "createTable(newTable_rawForwardIndex[\"tableName\"], 'Raw value forward index on start_location', newTable_rawForwardIndex)\n",
    "\n",
    "# Create a new table with sorted forward index with run-length encoding\n",
    "newTable_sortedForwardIndex = copy.deepcopy(table_config_template)\n",
    "newTable_sortedForwardIndex[\"tableName\"] = \"trips_sortedForwardIndex\"\n",
    "newTable_sortedForwardIndex[\"tableIndexConfig\"][\"sortedColumn\"] = [\"start_location\"]\n",
    "createTable(newTable_sortedForwardIndex[\"tableName\"], 'Sorted forward index with run-length encoding on start location', newTable_sortedForwardIndex)\n",
    "\n",
    "# Create a new table with bitmap inverted index\n",
    "newTable_bitmapInvertedIndex = copy.deepcopy(table_config_template)\n",
    "newTable_bitmapInvertedIndex[\"tableName\"] = \"trips_bitmapInvertedIndex_startLocation\"\n",
    "newTable_bitmapInvertedIndex[\"tableIndexConfig\"][\"invertedIndexColumns\"] = [\"start_location\"]\n",
    "createTable(newTable_bitmapInvertedIndex[\"tableName\"], 'Bitmap inverted index on start_location', newTable_bitmapInvertedIndex)\n",
    "\n",
    "# Create a new table with sorted inverted index\n",
    "newTable_sortedInvertedIndex = copy.deepcopy(table_config_template)\n",
    "newTable_sortedInvertedIndex[\"tableName\"] = \"trips_sortedInvertedIndex_startLocation\"\n",
    "newTable_sortedInvertedIndex[\"tableIndexConfig\"][\"invertedIndexColumns\"] = [\"start_location\"]\n",
    "newTable_sortedInvertedIndex[\"tableIndexConfig\"][\"sortedColumn\"] = [\"start_location\"]\n",
    "createTable(newTable_sortedInvertedIndex[\"tableName\"], 'Sorted inverted index on start_location', newTable_sortedInvertedIndex)\n",
    "\n",
    "# Create a new table with star tree index\n",
    "newTable_starTree = copy.deepcopy(table_config_template)\n",
    "newTable_starTree[\"tableName\"] = \"trips_starTreeIndex\"\n",
    "newTable_starTree[\"tableIndexConfig\"][\"starTreeIndexConfigs\"] = [{\n",
    "    \"dimensionsSplitOrder\": [\n",
    "      \"rider_is_premium\",\n",
    "      \"start_location_state\",\n",
    "      \"end_location\"\n",
    "    ],\n",
    "    \"functionColumnPairs\": [\n",
    "      \"SUM__payment_amount\",\n",
    "    ],\n",
    "    \"maxLeafRecords\": 1\n",
    "  }]\n",
    "createTable(newTable_starTree[\"tableName\"], 'Star Tree', newTable_starTree)\n",
    "\n",
    "# Create a new table with text index\n",
    "newTable_textIndex = copy.deepcopy(table_config_template)\n",
    "newTable_textIndex[\"tableName\"] = \"trips_textIndex\"\n",
    "newTable_textIndex[\"fieldConfigList\"]= [\n",
    "  {\n",
    "     \"name\":\"driver_name\",\n",
    "     \"encodingType\":\"RAW\",\n",
    "     \"indexType\":\"TEXT\"\n",
    "  },\n",
    "  {\n",
    "     \"name\":\"rider_name\",\n",
    "     \"encodingType\":\"RAW\",\n",
    "     \"indexType\":\"TEXT\"\n",
    "  }\n",
    "]\n",
    "newTable_textIndex[\"tableIndexConfig\"][\"noDictionaryColumns\"] = [\n",
    "     \"driver_name\",\n",
    "     \"rider_name\"\n",
    " ]\n",
    "createTable(newTable_textIndex[\"tableName\"], 'Text Index', newTable_textIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-phase",
   "metadata": {},
   "source": [
    "All created tables consume data from the same Kafka topic (`trips`). Therefore, all of them will contain the same data records.\n",
    "To ensure, that the consumption of the tables has finished before executing queries, we use the helper function from above to wait until the tables have finished loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# star-tree index building takes some time (longer than other tables), wait for it first\n",
    "wait_for_table_to_finish_loading(\"trips_starTreeIndex\")\n",
    "\n",
    "for table in table_list:\n",
    "    table_name = table[0]\n",
    "    if table_name == \"trips_starTreeIndex\":\n",
    "        continue\n",
    "    wait_for_table_to_finish_loading(table_name, wait_time=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read first start location and first driver name to use it as variables in upcoming queries\n",
    "query_result = query_sql(f\"SELECT start_location, driver_name FROM trips_default_index LIMIT 1\")\n",
    "\n",
    "startLocation = query_result['resultTable']['rows'][0][0]\n",
    "# Read the first name of the driver name\n",
    "driverName = (query_result['resultTable']['rows'][0][1]).split()[0]       \n",
    "\n",
    "print(f\"Using '{startLocation}' as start location and '{driverName}' as driver name for upcoming queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-estimate",
   "metadata": {},
   "source": [
    "## Table Size\n",
    "\n",
    "Although all tables contain the same amount of records and also the same record values, the table size differs.\n",
    "This is because of the different indexes used. E.g. a text index on two columns consumes much more space compared to the raw forward index. The Star-Tree index is allocating the most disk space, as Pinot materializes pre-aggregations for calculations on defined metric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_size_data = {}\n",
    "\n",
    "for table in table_list:\n",
    "    response = requests.get(f'http://pinot-controller.pinot:9000/tables/{table[0]}_REALTIME/size?detailed=false').json()\n",
    "    table_name = response['tableName']\n",
    "    table_size_data[table_name] = {\"description\": table[1], \"reportedSizeInMB\": response['reportedSizeInBytes']/1024/1024}\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "display(pd.DataFrame(table_size_data).transpose())\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-treaty",
   "metadata": {},
   "source": [
    "## Comparison of Indexing Options\n",
    "\n",
    "The function `executeSQLStatement` takes a query string and an array containing table names and index descriptions as input parameters. It executes the query on all tables which are defined in the array `table_list`. If `specific_tables_array` is empty, the query will be executed on all tables which have been created for this chapter. The top two records of the result data set are displayed once to get an insight into the result. Additionally, the function will create a `DataFrame` listing query execution statistics for each table. Metrics of one query execution will only be appended to the `DataFrame` if no exception occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeSQLStatement(sql_statement_with_variable, specific_tables_array):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    df_metrics = pd.DataFrame(columns=['indextype','table', 'numDocsScanned',\n",
    "       'numEntriesScannedInFilter', 'numEntriesScannedPostFilter',\n",
    "       'totalDocs', 'timeUsedMs',\n",
    "       'minConsumingFreshnessTimeMs',\n",
    "       'exceptions'])\n",
    "    b_resultRecordsNotShown = True;\n",
    "    if not specific_tables_array:\n",
    "        table_list_statement = table_list\n",
    "    else:\n",
    "        table_list_statement = specific_tables_array \n",
    "    for table in table_list_statement:\n",
    "    \n",
    "        sql_statement = sql_statement_with_variable.replace(\"XX_TABLE\",table[0])\n",
    "        sql_statement = sql_statement.replace(\"XX_STARTLOCATION\",\"'\"+startLocation+\"'\")\n",
    "        sql_statement = sql_statement.replace(\"XX_DRIVERNAME\",\"'\"+driverName+\"'\") \n",
    "        response = requests.post('http://pinot-broker.pinot:8099/query/sql', json={\n",
    "            \"sql\" : sql_statement\n",
    "        })\n",
    "        response_json=response.json()\n",
    "        d = {'indextype': table[1], 'table': table[0],'numDocsScanned': [response_json['numDocsScanned']],'numDocsScanned': [response_json['numDocsScanned']],'numEntriesScannedInFilter': [response_json['numEntriesScannedInFilter']], 'numEntriesScannedPostFilter':[response_json['numEntriesScannedPostFilter']],'totalDocs':[response_json['totalDocs']],'timeUsedMs':[response_json['timeUsedMs']],'minConsumingFreshnessTimeMs':[response_json['minConsumingFreshnessTimeMs']],'exceptions':[response_json['exceptions']]}\n",
    "        df_metrics_new = pd.DataFrame(data=d)\n",
    "        if not response_json['exceptions']:\n",
    "             df_metrics = df_metrics.append(df_metrics_new,ignore_index=True)\n",
    "       \n",
    "\n",
    "        if b_resultRecordsNotShown:\n",
    "            try:\n",
    "                if not response_json['exceptions']:\n",
    "                    columnNames = response_json['resultTable']['dataSchema']['columnNames']\n",
    "                    rows = response_json['resultTable']['rows']\n",
    "\n",
    "                    result_dataframe = pd.DataFrame(columns=columnNames,data=rows)\n",
    "                    print(\"Top two result records of: \" + sql_statement)\n",
    "                    display(result_dataframe.head(2))\n",
    "                    b_resultRecordsNotShown = False\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-fence",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "The main metrics of a query execution we will check are:\n",
    "- __timeUsedMs__: Total time between broker receiving the query request request and sending the response back to the client.\n",
    "- __numDocScanned__: Number of documents/records scanned while query processing. (Includes records scanned in the filter phase as well as after applying the filter.)\n",
    "- __numEntriesScannedInFilter__: It is an indicator of the latency contributed by the lookup phase. If this number is high, applying an index on the selection criteria might improve performance, especially if the selection criteria is highly selective.\n",
    "- __numEntriesScannedPostFilter__: High number is an indicator for low selectivity. Instead of regular indices, a star-tree index could help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-pregnancy",
   "metadata": {},
   "source": [
    "### Index Types\n",
    "\n",
    "For the tables create above, we configured the following index types:\n",
    "\n",
    "__Forward Index__\n",
    "- __Default Index: Dictionary-encoded forward index with bit compression__: \n",
    "    Apache Pinot will use this index by default for each column if no other index is configured in the table configuration. An id is assigned to each distinct value of the column, afterwards a dictionary is built matching an id to the value. In the forward index, only the bit-compressed id is persisted instead of the values. This compression improves space efficiency of the storage, if there are only a few distinct values.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/timebertt/adm-pinot/master/images/MarkdownTable_DefaultIndex.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-license",
   "metadata": {},
   "source": [
    "- __Raw Value Forward Index__: A raw value forward index is configured as a `noDictionaryColumn` in the table configuration. Instead of dictionary ids, the raw values will be stored in columns. Because of that, no dictionary lookup is required and due to the locality of values the performance of scanning large number of values is improved. \n",
    "- __Sorted forward index with run-length encoding__: The sorted forward index is applied on top of the dictionary-encoding. For each dictionary id, a start and end document id is stored. Only one sorted column can be configured per table.\n",
    "   \n",
    "__Inverted Index__: Inverted Indexes reduce the number of records which need be processed by identifying the ones which contain the search term. The inverted index is created by selecting all distinct values of a given column. For each value, a list of document ids which contain the value will be stored. If we search e.g. for \"Hessen\" as a state, we can look up the inverted index for \"Hessen\" and identify the documents in which that value appears. \n",
    "- __Bitmap inverted index__: A map from each value to a bitmap is maintained for the column which is enabled as bitmap inverted index (e.g. \"Th√ºringen\" -> `Doc5, Doc1`). If a column is used frequently for filtering, an inverted index will improve the performance. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/timebertt/adm-pinot/master/images/MarkdownTable_BitmapInvertedIndex.png\" width=\"35%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-observation",
   "metadata": {},
   "source": [
    "- __Sorted inverted index__: A sorted index can benefit from data locality, but can only be applied to one column.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/timebertt/adm-pinot/master/images/MarkdownTable_SortedInvertedIndex.png\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-builder",
   "metadata": {},
   "source": [
    "- __Star-Tree Index__: This index is built on multiple columns and pre-aggregates results per configured dimension hierarchy level, so that less values need to be processed. This can significantly improve query performance for hierarchical data (e.g. groups of users, workspaces, or states of locations in the `trips` example), on the other hand pre-aggregation requires also more disk space (table size can easily grow about twice of the size as the other tables).\n",
    "- __Text Index__: Text Indexes in Pinot allow to do aribtrary search on `STRING` columns (full text search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-sender",
   "metadata": {},
   "source": [
    "#### Default Index (Dictionary-encoded forward index with bit compression) vs Raw value forward index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-paraguay",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "executeSQLStatement(\"select count, driver_name, driver_rating, end_location, end_location_state from XX_TABLE WHERE start_location=XX_STARTLOCATION LIMIT 10000\",[['trips_default_index', 'Default Index (Dictionary-encoded forward index with bit compression) for each column'],['trips_rawForwardIndex', 'Raw value forward index on start_location']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-blink",
   "metadata": {},
   "source": [
    "The query execution on table `trips_rawForwardIndex` takes more time. The main difference between the two index types is, that the index on column `start_location` of `trips_default_index` creates a dictionary. This dictionary provides compression when values of the columns occurr repeatedly. \n",
    "A dictionary index can't provide this advantage over the other index, if the column values have a high cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-notion",
   "metadata": {},
   "source": [
    "#### Default Index (Dictionary-encoded forward index with bit compression) vs Sorted forward index with run-length encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQLStatement(\"select * from XX_TABLE WHERE start_location=XX_STARTLOCATION LIMIT 10000\", [['trips_default_index', 'Default Index (Dictionary-encoded forward index with bit compression) for each column'],['trips_sortedForwardIndex', 'Sorted forward index with run-length encoding on start location']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-communist",
   "metadata": {},
   "source": [
    "The sorted forward index on column `start_location` of table `trips_sortedForwardIndex` benefits from data locality. Because of this, `numEntriesScannedInFilter` is less than for the column with default index.\n",
    "Thus, query executions can be faster when using the sorted forward index on column `start_location`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-turkey",
   "metadata": {},
   "source": [
    "#### Default Index (Dictionary-encoded forward index with bit compression) vs Inverted index (Bitmap + Sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQLStatement(\"select driver_name, rider_name from XX_TABLE WHERE start_location=XX_STARTLOCATION LIMIT 10000\", [['trips_default_index', 'Default Index (Dictionary-encoded forward index with bit compression) for each column'],['trips_bitmapInvertedIndex_startLocation', 'Bitmap inverted index on start_location'],['trips_sortedInvertedIndex_startLocation','Sorted inverted index on start_location']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-magazine",
   "metadata": {},
   "source": [
    "As we can see, an inverted index can improve the query performance. In this case, no entries have to be scanned in the filtering phase and the query execution time is faster compared to using the dictionary encoded index.\n",
    "By using the sorted inverted index, the performance can benefit from data locality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-speed",
   "metadata": {},
   "source": [
    "#### Text Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-margin",
   "metadata": {},
   "source": [
    "A query searching selecting drivers by first name can only be executed successfully on table `trips_textIndex`, as it has a text index defined on column `driver_name`. The same query execution on other tables will fail, the metrics table only displays the executions without an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQLStatement(\"select * from XX_TABLE WHERE TEXT_MATCH ('driver_name',XX_DRIVERNAME) LIMIT 10000\", [['trips_textIndex','Text Index']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-library",
   "metadata": {},
   "source": [
    "#### Star-Tree Index\n",
    "\n",
    "The Start-Tree index utilizes pre-aggregation of results and is built on multiple columns. This index can improve the performance for specific queries, because the number of values to be processed is reduced by the pre-aggregation. Although usage of a Star-Tree index has the advantage of decreased query runtime, the table size on disk is significantly increased.\n",
    "For table `trips_starTreeIndex`, a Star-Tree index is built on the dimensions `rider_is_premium`, `start_location_state` and `end_location`. The sum of `payment_amount` is pre-aggregated and materialized based on the configured dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQLStatement(\"SELECT SUM(payment_amount) FROM XX_TABLE\",[[\"trips_default_index\",\"Default Index (Dictionary-encoded forward index with bit compression) for each column\"],[\"trips_starTreeIndex\", \"Start Tree\"]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-setting",
   "metadata": {},
   "source": [
    "When selecting the Star-Node without grouping by any dimension, Pinot doesn't need to access all documents. Instead, only the Star-Node of each segment is required. The reason, why `numDocsScanned` is not equal to the number of segments is, that there is always one segment that isn't completed yet. Pinot accesses each record of the consuming segment (status `IN_PROGRESS`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The table with the Star Tree Index consists of \" + str(len(requests.get('http://pinot-controller.pinot:9000/segments/trips_starTreeIndex').json()[0]['REALTIME'])) + \" segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-liver",
   "metadata": {},
   "source": [
    "Filtering on the dimension `rider_is_premium`, which builds the first node of the Star-Tree index, halves the number of `numDocsScanned`. This is because `rider_is_premiumn` is assigned randomly in our data generation, so there is a fifty percent chance that a rider is not premium and less documents of the consuming segment need to be scanned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "executeSQLStatement(\"SELECT SUM(payment_amount) FROM XX_TABLE WHERE rider_is_premium = 0\",[[\"trips_default_index\",\"Default Index (Dictionary-encoded forward index with bit compression) for each column\"],[\"trips_starTreeIndex\", \"Start Tree\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-friday",
   "metadata": {},
   "source": [
    "##### Trace Details For Star-Tree Index\n",
    "\n",
    "The trace details of the query execution display how much time was spent for which operator execution. We extract the operator details of the following query. The query executed on `trips_default_index` requires a lot of Aggregation Operators, as no data is pre-aggregated like it is the case for the table `trips_starTreeIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-consultancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trace_per_operator_from_result(result):\n",
    "    trace_data_per_operator = {}\n",
    "    for server, server_trace_json in result[\"traceInfo\"].items():\n",
    "        server_trace = json.loads(server_trace_json)\n",
    "        for trace_dict in server_trace:\n",
    "            for segment, segment_trace in trace_dict.items():\n",
    "                for segment_trace_element in segment_trace:\n",
    "                    for operator, operator_time in segment_trace_element.items():\n",
    "                        try:\n",
    "                            trace_data_per_operator[operator] += operator_time\n",
    "                        except KeyError:\n",
    "                            trace_data_per_operator[operator] = operator_time\n",
    "    \n",
    "    trace_data_array = []\n",
    "    for operator, operator_time in trace_data_per_operator.items():\n",
    "        trace_data_array.append({\"operator\": operator.replace(\" Time\", \"\"), \"time\": operator_time})\n",
    "    return pd.DataFrame(trace_data_array)\n",
    "\n",
    "display(extract_trace_per_operator_from_result(query_sql(\"SELECT SUM(payment_amount) FROM trips_starTreeIndex WHERE rider_is_premium = 0\")))\n",
    "\n",
    "display(extract_trace_per_operator_from_result(query_sql(\"SELECT SUM(payment_amount) FROM trips_default_index WHERE rider_is_premium = 0\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-shell",
   "metadata": {},
   "source": [
    "### Indexes - Comparison with other database technologies\n",
    "\n",
    "Two categories of the indexing options demonstrated above, that can also be found in traditional databases, are Forward Indexes and Inverted Indexes.\n",
    "Forward Indexes are frequently used in traditional database technologies as well to improve storage efficiency.\n",
    "Search Engines most often rely on a inverted index, like for example EleasticSearch.\n",
    "We saw, that there is a special index to do a fulltext search for records containing a specific string.\n",
    "\n",
    "We also demonstrated two other indexing techniques, that are typically not offered by traditional database systems: raw value forward index and Star-Tree index.\n",
    "The raw value forward index doesn't include dictionaries - when aggregating a large number of values, it can take advantage of data locality for scanning.\n",
    "\n",
    "The Star-Tree Index is an important and special concept for Pinot, because it is utilizes pre-aggregation for group-by queries to achieve low query latencies. It is specifically designed for the analytical use cases which Pinot was built for and makes it a a key differenciator of Pinot.\n",
    "E.g. Star-Tree Indexes can bring great benefits, if there is the requirement to return data e.g. per user level - like it is the case for the \"Who viewed my Profile\" application at LinkedIn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-graph",
   "metadata": {},
   "source": [
    "# Closing Remarks\n",
    "__TBD__:\n",
    "Compared to known databases, complex set up and configurations.\n",
    "The several different components ensure the flexible scale up of the cluster and the high availability, as the system would continue to server queries also if one node goes down. This is a big advantage, but the different also add more complexitiy to the whole landscape. \n",
    "\n",
    "We experienced the advantages of Pinot, e.g. the possibility to dynamically change configurations and to quickly create new tables consuming data from the Kafka topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-newman",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [Rogers, Ryan & Subramaniam, Subbu & Peng, Sean & Durfee, David & Lee, Seunghyun & Kancha, Santosh & Sahay, Shraddha & Ahammad, Parvez. (2020). LinkedIn's Audience Engagements API: A Privacy Preserving Data Analytics System at Scale. https://arxiv.org/pdf/2002.05839.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-bhutan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
