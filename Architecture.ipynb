{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedicated-reputation",
   "metadata": {},
   "source": [
    "# Introduction to Pinot\n",
    "## Use Cases\n",
    "Pinot is a distributed high-available OLAP datastore and built to serve analytical queries on real-time event data. It was developed by engineers of LinkedIn and Uber.\n",
    "LinkedIn is operating Pinot clusters for real-time Online Analytical Processing. They divide their analytics applications into two main categories in their solution landscape: Internal applications and site-facing applications. Internal applications need to process large data volume (trillions of records), but for them smaller query latencies are tolerated. On the opposite, site-facing applications are available for hundreds of millions of LinkedIn members. These applications have a very high query volume and are expected to have a lower latency.\n",
    "Pinot production clusters at LinkedIn are serving tens of thousands queries per second. Overall, more than 50 analytical use cases are supported, and over millions of records are ingested per second. \n",
    "[Rogers, Ryan & Subramaniam, Subbu & Peng, Sean & Durfee, David & Lee, Seunghyun & Kancha, Santosh & Sahay, Shraddha & Ahammad, Parvez. (2020). LinkedIn's Audience Engagements API: A Privacy Preserving Data Analytics System at Scale. https://arxiv.org/pdf/2002.05839.pdf]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-sleeve",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Multiple distributed system components build a Pinot cluster. Each Pinot cluster consists of a controller, one or multiple brokers and multiple servers. Pinot supports multi-tenancy out-of-the-box, as multiple brokers and servers can be combined. A table in pinot consists of columns and rows. The horizontally division into shards is named segments. \n",
    "\n",
    "Apache Helix is a generic cluster management framework which is used for automatic management of partitioned and replicated distributed systems by creating and assigning tasks. Apache Zookeeper takes care of coordination and maintenance of the overall cluster state and health. In addition, it stores information about the cluster like server locations of a segment and table schema information. The Controller embeds the Helix agent and is the driver of the cluster. To access CRUD (Create, Read, Update, Delete) Operations on logical storage resources, it provides a REST interface.\n",
    "\n",
    "If a client wants to query data of Pinot tables, the request will be sent to the broker. It routes queries to the appropriate server instances and keeps track on the query routing tables. These routing tables consist of a mapping between segments and the server the segment resides on. This ensures the right routing of the query to the correct segment. Either these segments contain real-time data, or data is pushed into offline segments. By default, the query load is balanced across all available servers. The broker will return one consolidated reply to the client, independent from the fact if the table is divided into real-time and offline segments.\n",
    "\n",
    "Servers are categorized into offline and real-time servers. According to this categorization, servers in Pinot either host offline or realtime data. The responsibility of a server is defined by the table assignment strategy.\n",
    "\n",
    "If a new real-time table is configured, the real-time server will start consuming data from the streaming source. This can be for example a Kafka topic. The broker will watch the consumption, detect new segments and maintain them in the query routing list. If a segment has been completed (reached a specific amount of records or was available for a specific timeframe), the controller will upload the segment to the cluster's segment store. The status of the uploaded segment changes from \"consuming\" to \"online\" and the controller will start a new consumption on the realtime server.\n",
    "With batch ingestion, already existing data (e.g. in Hadoop) can be loaded to a Pinot table. \n",
    "\n",
    "\n",
    "<img src='https://gblobscdn.gitbook.com/assets%2F-LtH6nl58DdnZnelPdTc%2F-M1pSGleddLn2q1vYEeM%2F-M1pvo4yOL0qNSjSS5nc%2FPinot-architecture%20(1).svg?alt=media&token=b0d011d8-4457-4bea-b29d-55d409eae7df' width=\"35%\" height=\"35%\">\n",
    "                                                 \n",
    "Source of the screenshot: https://docs.pinot.apache.org/basics/architecture (accessed 4 April 2021)\n",
    "\n",
    "In addition to the components on the screenshot showing the Pinot cluster architecture, minions can be deployed to the cluster. They leverage Apache Helix and execute tasks which are provided by the Helix Task Executor Framwork. A minion takes over tasks with intensive workloads from other components like indexing or purging data from a Pinot cluster, for example due to GDPR compliance.\n",
    "The Pinot minion is also required for the Offline Flow in Pinot. This flow moves records from  REALTIME tables to according OFFLINE tables.\n",
    "\n",
    "\n",
    "### API Interface for Broker and Controller\n",
    "Requests for queries via the REST API are sent to port 8099, as the broker is running on this port in our use case.\n",
    "To get information about the resources of the Pinot cluster, we are accessing the controller, which is running on port 9000.\n",
    "Broker Configurations are defined in a specific broker.conf file. The properties define configurations like the query port for the broker or a limit for queries. Latter has the purpose to protect brokers and servers against queries returning very large amount of records. A query limit needs to be enabled at cluster level. In our scenario, parameter pinot.broker.enable.query.limit.override is set to false, this means, the broker won't override or add a query limit when the returned record amount is larger than defined in the broker config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "committed-reverse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBroker: \u001b[0m{\n",
      "  \"DefaultTenant\": [\n",
      "    {\n",
      "      \"instanceName\": \"Broker_pinot-broker-0.pinot-broker-headless.pinot.svc.cluster.local_8099\",\n",
      "      \"host\": \"Broker_pinot-broker-0.pinot-broker-headless.pinot.svc.cluster.local\",\n",
      "      \"port\": 8099\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[1mHealth of Controller: \u001b[0mGOOD\n",
      "\u001b[1mCluster: \u001b[0m{\n",
      "  \"allowParticipantAutoJoin\": \"true\",\n",
      "  \"enable.case.insensitive\": \"false\",\n",
      "  \"pinot.broker.enable.query.limit.override\": \"false\",\n",
      "  \"default.hyperloglog.log2m\": \"8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"\\033[1m\" + \"Broker: \"+ \"\\033[0m\" + json.dumps((requests.get('http://pinot-controller.pinot:9000/v2/brokers/tenants')).json(), indent=2))\n",
    "print(\"\\033[1m\" + \"Health of Controller: \"+ \"\\033[0m\" + requests.get('http://pinot-controller.pinot:9000/pinot-controller/admin').text)\n",
    "print(\"\\033[1m\" + \"Cluster: \"+ \"\\033[0m\" + json.dumps((requests.get('http://pinot-controller.pinot:9000/cluster/configs')).json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-contest",
   "metadata": {},
   "source": [
    "### Comparison with known database technologies\n",
    "\n",
    "In Pinot, data ingestion is append-only. There is no possibility to modify values after ingestion by doing operations like UPSERT known from databases like PostgreSQL. Pinot is no replacement for databases in an operational business environment, which usually require updates to data because of the event's nature or due to data correction. For this use cases, Pinot does not fit. Instead, it can enhance use cases requiring fast analytics. To enable data purging, the Minion can be used in Pinot.\n",
    "Another limitation of Apache Pinot compared to databases like PostgreSQL is that it doesn't support queries requiring movements of large amounts of data between the nodes, like joins. The query engine Presto can be used to join different tables in Pinot, but Presto needs to be set up on its own and is not part of Pinot.\n",
    "\n",
    "Tables in Pinot can have one primary time column, which is used to manage the time boundary between offline and realtime data in a hybrid table. This may sound familiar to the known concept of time series databases like Influxdb. Both databases are built to handle events with a time stamp -  the time stamp is not a must for Pinot but realizes the hybrid tables. In addition, Pinot is not only focused on storing metrics, despite numeric data types and date time fields there is the option to have columns of type string and bytes. Although Influxdb also support Strings to a specific extend, Pinot offers possibilities for e.g. text search.\n",
    "\n",
    "Compared to the time-series database Influxdb, Pinot is optimized for storing time data with a focus on write operations and queries. Updates and deletion operations are not supported in Apache Pinot, despite upserting data via stream ingestion - if a primary key has been defined in the schema.\n",
    "\n",
    "Pinot can be categorized as a homogeneous distributed database, as all sites have identical software and access to the same defined schemas. In addition, all components are aware of each other and cooperate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-magnet",
   "metadata": {},
   "source": [
    "## Introduction to Schemas and Tables in Pinot\n",
    "### Schemas\n",
    "To create a table in Pinot, a schema is required. A schema configuration defines fields and data types, this metadata is stored in the Zookeeper.\n",
    "In our example, our data is based on a fictional online plattform which connects car drivers and passengers to travel together in Germany. \n",
    "\n",
    "Columns in Pinot consist of different categories: \n",
    "- Dimension columns support operations like GROUP BY and WHERE. (e.g. name of the car driver, license plate)\n",
    "- Metric columns represent quantitative data and can be used e.g. for aggregation and filter clauses. (e.g. payment amount, rating of the driver)\n",
    "- DateTime columns represent time columns. One DataTime column can be treated as the primary time column, which is defined in the segment config part of a table. The primary time column is used for offline streams between offline and realtime data in a realtime table. Operations supported are e.g. GROUP BY and WHERE. (e.g. time when the car driver was requested by the rider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italic-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Schema: {\"status\":\"trips successfully added\"}\n",
      "Get all schemas: ['trips']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "schemaConfiguration = {\n",
    "  \"schemaName\": \"trips\",\n",
    "  \"dimensionFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"rider_name\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"driver_name\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"license_plate\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"start_location\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"start_zip_code\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "     {\n",
    "      \"name\": \"start_location_state\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    }, \n",
    "    {\n",
    "      \"name\": \"end_location\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"end_zip_code\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "      {\n",
    "      \"name\": \"end_location_state\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    }, \n",
    "    {\n",
    "      \"name\": \"rider_is_premium\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"metricFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"count\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"defaultNullValue\": 1\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"payment_amount\",\n",
    "      \"dataType\": \"FLOAT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"payment_tip_amount\",\n",
    "      \"dataType\": \"FLOAT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"trip_wait_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"rider_rating\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"driver_rating\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"dateTimeFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"trip_start_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"PRIMARY\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"request_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"SECONDARY\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"trip_end_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"SECONDARY\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Create Schema\n",
    "response = requests.post('http://pinot-controller.pinot:9000/schemas?override=false', json=schemaConfiguration)\n",
    "print(\"Create Schema: \" + response.text)\n",
    "# Display all Schemas\n",
    "response = (requests.get('http://pinot-controller.pinot:9000/schemas')).json()\n",
    "print(\"Get all schemas: \" + str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-consciousness",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "Our Pinot tables will consume data from a Kafka Topic in realtime. To be able to consume messages of this topic, data needs to be produced and sent to the topic before.\n",
    "\n",
    "To create a Kafka Topic, we first need to create a Kafka Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "therapeutic-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"pinot-kafka.pinot:9092\", \n",
    "    client_id='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-refund",
   "metadata": {},
   "source": [
    "The below function generates data records for car rides in Germany and inserts them to the Kafka Topic. Each ride consists of driver and passenger details, such as name and rating, measures like payments, details about origin and destination of the trip and different time measures, for example the time stamp when the trip was requested. Date and time of the trip is generated based on the current timestamp, adding up some time randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "formed-likelihood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records generated\n",
      "1 records generated\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaProducer\n",
    "import csv\n",
    "import random\n",
    "import names\n",
    "import time\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['pinot-kafka.pinot:9092'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "# Choose random city of file containing German cities with postcode\n",
    "if not os.path.exists(\"./pgeocodeDE.txt\"):\n",
    "    # download segment to local file\n",
    "    response = requests.get(\"https://symerio.github.io/postal-codes-data/data/geonames/DE.txt\")\n",
    "    with open(\"./pgeocodeDE.txt\", 'w',encoding='utf8') as out_file:\n",
    "        out_file.write(response.text)\n",
    "    del response\n",
    "\n",
    "geocode_file = open('./pgeocodeDE.txt')\n",
    "geocode_list = list(csv.reader(geocode_file, delimiter='\\t'))[1:] # skip first line (header)\n",
    "random.shuffle(geocode_list)\n",
    "geocode_list = geocode_list[:1000] # take only random 1000 places to generate more overlapping data\n",
    "geocode_file.close()\n",
    "\n",
    "def choose_random_city():\n",
    "    return random.choice(geocode_list)\n",
    "\n",
    "# generate only 1000 driver/rider names to generate more overlapping data\n",
    "names_list = []\n",
    "for i in range(1000):\n",
    "    names_list.append(names.get_full_name())\n",
    "\n",
    "def choose_random_name():\n",
    "    return random.choice(names_list)\n",
    "    \n",
    "# Generation of License Plate\n",
    "# create a pool of letters to choose from\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "numbers = '0123456789'\n",
    "\n",
    "def generate_license_plate():\n",
    "    # generate 3 randomly chosen letters, L1, L2, L3\n",
    "    L1 = random.choice(letters)\n",
    "    L2 = random.choice(letters)\n",
    "    L3 = random.choice(letters)\n",
    "    L4 = random.choice(letters)\n",
    "    # generate 4 randomly chosen numbers, N1, N2, N3, N4\n",
    "    N1 = random.choice(numbers)\n",
    "    N2 = random.choice(numbers)\n",
    "  \n",
    "    # combine it together into one print function\n",
    "    return(L1+L2+'-'+L3+L4+'-'+N1+N2)\n",
    "\n",
    "# Calculation of price based on distance between start city and end destination\n",
    "def calculate_price(v_distance):\n",
    "    v_multiplicator=round(random.uniform(0.8, 2.0),2)\n",
    "    v_price=round(v_distance*v_multiplicator,2)\n",
    "    return(v_price)\n",
    "\n",
    "# begin generating trips data at current time\n",
    "start_timestamp_ms = time.time_ns() // 1000000\n",
    "\n",
    "# Generate data\n",
    "num_records = 100000 + random.randint(5000,10000)\n",
    "for i in range(num_records):\n",
    "    v_start_location=choose_random_city()\n",
    "    v_end_location=choose_random_city()\n",
    "    v_distance = random.randint(5,1000)\n",
    "\n",
    "    # add random jitter, in large system our event stream is probably also not strictly sorted\n",
    "    v_requesttime = start_timestamp_ms + i*1000 + random.randint(0,100);\n",
    "\n",
    "    v_waiting_time_millis = random.randint(1,3600000)\n",
    "    v_trip_time = round((v_distance/random.randint(45,60)) * 60 *60*1000)\n",
    "\n",
    "    record = {\n",
    "        \"rider_name\": choose_random_name(),\n",
    "        \"driver_name\": choose_random_name(),\n",
    "        \"license_plate\":generate_license_plate(),\n",
    "        \"start_location\": v_start_location[2],\n",
    "        \"start_zip_code\": v_start_location[1],\n",
    "        \"start_location_state\": v_start_location[3],\n",
    "        \"end_location\": v_end_location[2],\n",
    "        \"end_zip_code\": v_end_location[1],\n",
    "        \"end_location_state\": v_end_location[3],\n",
    "        \"rider_is_premium\": random.randint(0, 1),\n",
    "        \"count\": 1,\n",
    "        \"payment_amount\": calculate_price(v_distance),\n",
    "        \"payment_tip_amount\": random.randint(5,50),\n",
    "        \"trip_wait_time_millis\": v_waiting_time_millis,\n",
    "        \"rider_rating\": random.randint(0,5),\n",
    "        \"driver_rating\": random.randint(0,5),\n",
    "        \"trip_start_time_millis\": v_requesttime+v_waiting_time_millis,\n",
    "        \"request_time_millis\": v_requesttime,\n",
    "        \"trip_end_time_millis\": v_requesttime+v_waiting_time_millis+v_trip_time\n",
    "    }\n",
    "    producer.send('trips', value=record)\n",
    "    \n",
    "    if i % 5000 == 0:\n",
    "        print(f'{i} records generated')\n",
    "print(f'{num_records} records generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-chassis",
   "metadata": {},
   "source": [
    "## Tables\n",
    "Tables represent a collection of related data in Pinot. A table can have the type OFFLINE (ingesting pre-built pinot-segments from external stores), REALTIME (data ingestion from streams) or HYBRID (table consists of OFFLINE and REALTIME tables). The use is not require to know the type of a table when executing a table. \n",
    "To configure a table, properties like name, type and indexing are required. In the following example, the table is consuming data from the Kafka Topic *trips*. \n",
    "One feature of Pinot, contributing to performance improvements, is the pre-aggregation. With that, realtime stream data is aggregated when its consume to reduce segment sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "concrete-affect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\"status\":\"Table trips_aggregate_REALTIME succesfully added\"}\n"
     ]
    }
   ],
   "source": [
    "json_tableConfig = {\n",
    "  \"tableName\": \"trips_aggregate\",\n",
    "  \"tableType\": \"REALTIME\",\n",
    "  \"segmentsConfig\": {\n",
    "    \"timeColumnName\": \"trip_start_time_millis\",\n",
    "    \"timeType\": \"MILLISECONDS\",\n",
    "    \"retentionTimeUnit\": \"DAYS\",\n",
    "    \"retentionTimeValue\": \"60\",\n",
    "    \"schemaName\": \"trips\",\n",
    "    \"replication\": \"1\",\n",
    "    \"replicasPerPartition\": \"1\"\n",
    "  },\n",
    "  \"tenants\": {},\n",
    "  \"tableIndexConfig\": {\n",
    "    \"loadMode\": \"MMAP\",\n",
    "    \"streamConfigs\": {\n",
    "      \"streamType\": \"kafka\",\n",
    "      \"stream.kafka.consumer.type\": \"simple\",\n",
    "      \"stream.kafka.topic.name\": \"trips\",\n",
    "      \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n",
    "      \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n",
    "      \"stream.kafka.zk.broker.url\": \"pinot-kafka-zookeeper:2181\",\n",
    "      \"stream.kafka.broker.list\": \"pinot-kafka:9092\",\n",
    "      \"realtime.segment.flush.threshold.time\": \"12h\",\n",
    "      \"realtime.segment.flush.threshold.size\": \"20000\",\n",
    "      \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\"\n",
    "    },\n",
    "      \"noDictionaryColumns\": [\"count\", \"payment_amount\", \"payment_tip_amount\",\"trip_wait_time_millis\", \"rider_rating\", \"driver_rating\"],\n",
    "      \"aggregateMetrics\": True,\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"customConfigs\": {}\n",
    "  }\n",
    "} \n",
    "\n",
    "response = requests.post('http://pinot-controller.pinot:9000/tables', json=json_tableConfig)\n",
    "print(response)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-liberia",
   "metadata": {},
   "source": [
    "After creation, data records of the Kafka Topic are loaded by the table. To execute a query, the statement string is sent to the broker of the Pinot cluster. The response contains the result records, as well as details about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "killing-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resultTable': {'dataSchema': {'columnNames': ['count(*)'], 'columnDataTypes': ['LONG']}, 'rows': [[308641]]}, 'exceptions': [], 'numServersQueried': 1, 'numServersResponded': 1, 'numSegmentsQueried': 16, 'numSegmentsProcessed': 16, 'numSegmentsMatched': 16, 'numConsumingSegmentsQueried': 1, 'numDocsScanned': 308641, 'numEntriesScannedInFilter': 0, 'numEntriesScannedPostFilter': 0, 'numGroupsLimitReached': False, 'totalDocs': 308641, 'timeUsedMs': 6, 'segmentStatistics': [], 'traceInfo': {}, 'minConsumingFreshnessTimeMs': 1618089542751}\n"
     ]
    }
   ],
   "source": [
    "print(requests.post('http://pinot-broker.pinot:8099/query/sql', json={\n",
    "            \"sql\" : \"SELECT COUNT(*) FROM trips_aggregate\"\n",
    "    }).json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
