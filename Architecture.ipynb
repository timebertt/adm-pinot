{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yellow-writing",
   "metadata": {},
   "source": [
    "# Introduction to Pinot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-princeton",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "Pinot is an open-source distributed highly-available OLAP datastore and built to serve analytical queries on real-time event data. It is developed by engineers of LinkedIn and Uber.\n",
    "LinkedIn is operating Pinot clusters for real-time Online Analytical Processing. They divide their analytics applications into two main categories in their solution landscape: Internal applications and user-facing applications. Internal applications need to process large data volume (trillions of records), but higher query latencies are tolerated. On the opposite, user-facing applications are available for hundreds of millions of LinkedIn members. These applications have a very high query volume and are expected to have a lower latency.\n",
    "Pinot production clusters at LinkedIn are serving tens of thousands queries per second. Overall, more than 50 analytical use cases are supported, and millions of records are ingested per second. \n",
    "[Rogers, Ryan & Subramaniam, Subbu & Peng, Sean & Durfee, David & Lee, Seunghyun & Kancha, Santosh & Sahay, Shraddha & Ahammad, Parvez. (2020). LinkedIn's Audience Engagements API: A Privacy Preserving Data Analytics System at Scale. https://arxiv.org/pdf/2002.05839.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-hawaiian",
   "metadata": {},
   "source": [
    "## Design Principles\n",
    "\n",
    "Key requirements for Pinot include:\n",
    "\n",
    "- high performance (low latency) query execution\n",
    "- near-realtime data ingestion\n",
    "- linear horizontal scalability (in terms of data size, ingestion rate and query rate)\n",
    "- query flexibility to cover a wide range of analytical use cases\n",
    "- high availability of data as well as components (fault tolerance)\n",
    "\n",
    "All of these requirements influence Pinot's fundamental design principles and distributed architecture. We present, how Pinot manages to achieve these goals in the following sections by describing the core concepts and demonstrating the most important mechanisms in Pinot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "awful-second",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "A Pinot cluster is comprised of multiple distributed components. Each Pinot cluster consists of a controller, one or multiple brokers and multiple servers. Pinot supports multi-tenancy out-of-the-box, as multiple brokers and servers can be assigned to serve specific tenants. A table in pinot consists of columns and rows, which are broken horizontally into shards (named segments).\n",
    "\n",
    "Apache Helix is a generic cluster management framework which is used for automatic management of partitioned and replicated distributed systems by creating and assigning tasks. Apache Zookeeper takes care of coordination and maintenance of the overall cluster state and health. In addition, it stores information about the cluster like server locations of a segment and table schema information. The Controller embeds the Helix agent and is the driver of the cluster. To access CRUD (Create, Read, Update, Delete) Operations on logical storage resources, it provides a REST interface.\n",
    "\n",
    "If a client wants to query data of Pinot tables, the request will be sent to the broker. It routes queries to the appropriate server instances and keeps track on the query routing tables. These routing tables consist of a mapping between segments and server, where the segments reside on. This ensures the right routing of the query to the correct segment. Segments can either consume real-time data or data can be pushed into offline segments. By default, the query load is balanced across all available servers. The broker will return one consolidated result to the client, independent from the fact whether the table is divided into real-time and offline segments.\n",
    "\n",
    "Servers are categorized into offline and real-time servers. According to this categorization, servers in Pinot either host offline or real-time data. The responsibility of a server is defined by the table assignment strategy.\n",
    "\n",
    "If a new real-time table is configured, the real-time server will start consuming data from the streaming source (e.g. Kafka topic). The broker will watch the consumption, detect new segments and maintain them in the query routing list. If a segment has been completed (reached a specific amount of records or was available for a specific timeframe), the controller will upload the segment to the cluster's segment store. The status of the uploaded segment changes from \"consuming\" to \"online\" and the controller will start a new consumption on the real-time server.\n",
    "With batch ingestion, already existing data (e.g. in Hadoop) can be loaded to a Pinot table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-california",
   "metadata": {},
   "source": [
    "<img src='https://gblobscdn.gitbook.com/assets%2F-LtH6nl58DdnZnelPdTc%2F-M1pSGleddLn2q1vYEeM%2F-M1pvo4yOL0qNSjSS5nc%2FPinot-architecture%20(1).svg?alt=media&token=b0d011d8-4457-4bea-b29d-55d409eae7df' width=\"35%\" height=\"35%\">\n",
    "                                                 \n",
    "Image source: https://docs.pinot.apache.org/basics/architecture (accessed April, 4th 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-vatican",
   "metadata": {},
   "source": [
    "In addition to components shown in the above architectural diagram, minions can be deployed to the cluster. They leverage Apache Helix and execute tasks which are provided by the Helix Task Executor Framwork. A minion takes over tasks with intensive workloads from other components like indexing or purging data from a Pinot cluster, for example due to GDPR compliance.\n",
    "The Pinot minion can also be used for Pinot's Offline Flow, which moves records from REALTIME tables to corresponding OFFLINE tables (covered later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-symposium",
   "metadata": {},
   "source": [
    "### API Interface for Broker and Controller\n",
    "\n",
    "Queries are sent to the broker's REST API (listening on port 8099 by default).\n",
    "To get information about the resources of the Pinot cluster, we are accessing the controller's REST API, which is listening on port 9000.\n",
    "Broker Configurations are defined in a specific broker.conf file. The properties define configurations like the query port for the broker or a limit for queries. The latter of which has the purpose to protect brokers and servers against queries returning very large amount of records. A query limit needs to be enabled at cluster level. In our scenario, the parameter `pinot.broker.enable.query.limit.override` is set to false, which means that the broker won't override or add a query limit when the returned record amount is larger than defined in the broker config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "other-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBroker: \u001b[0m{\n",
      "  \"DefaultTenant\": [\n",
      "    {\n",
      "      \"instanceName\": \"Broker_pinot-broker-0.pinot-broker-headless.pinot.svc.cluster.local_8099\",\n",
      "      \"host\": \"Broker_pinot-broker-0.pinot-broker-headless.pinot.svc.cluster.local\",\n",
      "      \"port\": 8099\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[1mHealth of Controller: \u001b[0mGOOD\n",
      "\u001b[1mCluster: \u001b[0m{\n",
      "  \"allowParticipantAutoJoin\": \"true\",\n",
      "  \"enable.case.insensitive\": \"false\",\n",
      "  \"pinot.broker.enable.query.limit.override\": \"false\",\n",
      "  \"default.hyperloglog.log2m\": \"8\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"\\033[1m\" + \"Broker: \"+ \"\\033[0m\" + json.dumps((requests.get('http://pinot-controller.pinot:9000/v2/brokers/tenants')).json(), indent=2))\n",
    "print(\"\\033[1m\" + \"Health of Controller: \"+ \"\\033[0m\" + requests.get('http://pinot-controller.pinot:9000/pinot-controller/admin').text)\n",
    "print(\"\\033[1m\" + \"Cluster: \"+ \"\\033[0m\" + json.dumps((requests.get('http://pinot-controller.pinot:9000/cluster/configs')).json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-prevention",
   "metadata": {},
   "source": [
    "### Key differences to well-known database technologies\n",
    "\n",
    "In Pinot, data ingestion is append-only. There is no possibility to modify values after ingestion by doing operations like `UPDATE` known from databases like PostgreSQL. Pinot is no replacement for databases in an operational business environment, which usually require updates to data because of the event's nature or due to data correction. For this use cases, Pinot does not fit. Instead, it can enhance use cases requiring fast analytics. However, data can still be purged after ingestion for fullfilling compliance requirements (e.g. GDPR). For this, the Minion can be used to replace entire segments, but in no case, single records can be manipulated.\n",
    "\n",
    "Another difference of Apache Pinot compared to databases like PostgreSQL is that it doesn't support queries requiring movements of large amounts of data between the nodes, like joins. The query engine Presto can be used to join different tables in Pinot, but Presto needs to be set up additionally and is not part of Pinot.\n",
    "\n",
    "Tables in Pinot typically have one primary time column, which is used to manage the time boundary between offline and realtime data in a hybrid table. This may sound familiar to the known concept of time series databases like Influxdb. Both databases are built to handle events with a timestamp, but the timestamp in Pinot is only strictly required for hybrid tables. In addition, Pinot is not only focused on storing timeseries of metrics, it also offers to storm string and bytes values in addition to numeric data types and date time fields. Although Influxdb also support strings to a specific extend, Pinot also offers e.g. text indexing for enhanced full text search.\n",
    "Compared to the timeseries databases like Influxdb, Pinot is optimized for storing time data with a focus on append operations and queries. Update and delete operations on single records are not supported in Apache Pinot, though stream ingestion supports upserts, if a primary key has been defined in the schema.\n",
    "\n",
    "Another key difference of Pinot in comparison to other distributed databases is the heterogeneous nature of its components. Some traditional RDBMSs like for example PostgreSQL can be scaled horizontally to form a cluster by adding more instances, that will each store and manage different partitions (shards) of the dataset. In this case, such a distributed setup is comprised of only a single stateful component, which is started on multiple machines (homogeneous distributed system).\n",
    "In contrast to this, a Pinot cluster is comprised multiple heterogeneous components (described above), which each serve a specific purpose and are only responsible for a given subtask of the entire system. For example, servers are the stateful components of Pinot, that store and query the actual dataset, while brokers are stateless components, that don't host data themselves and only serve the query frontend for the database. With this, Pinot can be seen as a heterogeneous distributed system, which makes it more complex to deploy and operate, but also serves the key requirements described above (mainly horizontal scalability and fault tolerance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-governor",
   "metadata": {},
   "source": [
    "## Schemas and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-large",
   "metadata": {},
   "source": [
    "### Schemas\n",
    "\n",
    "To create a table in Pinot, a schema is required. A schema configuration defines fields and data types, this metadata is stored in the Zookeeper.\n",
    "In our examples, we work with data of a fictional online plattform which connects car drivers and passengers to travel together in Germany (ride sharing). \n",
    "\n",
    "Columns in Pinot are of different categories: \n",
    "- dimension columns: support operations like `GROUP BY` and `WHERE` (\"slice and dice\"), e.g. name of the car driver, trip start and end location\n",
    "- metric columns: represent quantitative data and can be used e.g. for aggregation clauses (e.g. payment amount, rating of the driver)\n",
    "- DateTime columns: represent timestamps of records. One DataTime column can be treated as the primary time column, which is defined in the segment config of a table. The primary time column is used for determining boundaries of segments and between offline and realtime data in hybrid tables. A typical operation on DateTime columns is for example `WHERE`, e.g. time when the a ride was requested by the rider\n",
    "\n",
    "Let's define the example `trips` schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "binding-proceeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Schema: {\"status\":\"trips successfully added\"}\n",
      "Get all schemas: ['trips']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "schemaConfiguration = {\n",
    "  \"schemaName\": \"trips\",\n",
    "  \"dimensionFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"rider_name\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"driver_name\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"license_plate\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"start_location\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"start_zip_code\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "     {\n",
    "      \"name\": \"start_location_state\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    }, \n",
    "    {\n",
    "      \"name\": \"end_location\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"end_zip_code\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    },\n",
    "      {\n",
    "      \"name\": \"end_location_state\",\n",
    "      \"dataType\": \"STRING\",\n",
    "      \"defaultNullValue\": \"\"\n",
    "    }, \n",
    "    {\n",
    "      \"name\": \"rider_is_premium\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"metricFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"count\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"defaultNullValue\": 1\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"payment_amount\",\n",
    "      \"dataType\": \"FLOAT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"payment_tip_amount\",\n",
    "      \"dataType\": \"FLOAT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"trip_wait_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"rider_rating\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"driver_rating\",\n",
    "      \"dataType\": \"INT\",\n",
    "      \"defaultNullValue\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"dateTimeFieldSpecs\": [\n",
    "    {\n",
    "      \"name\": \"trip_start_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"PRIMARY\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"request_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"SECONDARY\"\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"trip_end_time_millis\",\n",
    "      \"dataType\": \"LONG\",\n",
    "      \"format\": \"1:MILLISECONDS:EPOCH\",\n",
    "      \"granularity\": \"1:MINUTES\",\n",
    "      \"dateTimeType\": \"SECONDARY\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# create the trips schema\n",
    "response = requests.post('http://pinot-controller.pinot:9000/schemas?override=true', json=schemaConfiguration)\n",
    "print(\"Create Schema: \" + response.text)\n",
    "\n",
    "# list all Schemas\n",
    "response = (requests.get('http://pinot-controller.pinot:9000/schemas')).json()\n",
    "print(\"Get all schemas: \" + str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-louisville",
   "metadata": {},
   "source": [
    "### Data Generation\n",
    "\n",
    "Our Pinot tables will consume data from a Kafka Topic in realtime. To be able to consume messages from this topic, data needs to be produced and sent to the topic before.\n",
    "\n",
    "To create and fill our Kafka topic, we first need to create a Kafka producer client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "secondary-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=['pinot-kafka.pinot:9092'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-attachment",
   "metadata": {},
   "source": [
    "The below functions are used to generate random data records for car rides in Germany and inserts them to the Kafka Topic. Each ride consists of driver and passenger details, such as name and rating, measures like payments, details about origin and destination of the trip and different time measures, for example the time stamp when the trip was requested. Date and time of the trip is generated based on the current timestamp (and advancing by roughly 1 second per record)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "alleged-madison",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import names\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Choose random city of file containing German cities with postcode\n",
    "if not os.path.exists(\"./pgeocodeDE.txt\"):\n",
    "    # download segment to local file\n",
    "    response = requests.get(\"https://symerio.github.io/postal-codes-data/data/geonames/DE.txt\")\n",
    "    with open(\"./pgeocodeDE.txt\", 'w',encoding='utf8') as out_file:\n",
    "        out_file.write(response.text)\n",
    "    del response\n",
    "\n",
    "geocode_file = open('./pgeocodeDE.txt')\n",
    "geocode_list = list(csv.reader(geocode_file, delimiter='\\t'))[1:] # skip first line (header)\n",
    "random.shuffle(geocode_list)\n",
    "geocode_list = geocode_list[:1000] # take only random 1000 places to generate more overlapping data\n",
    "geocode_file.close()\n",
    "\n",
    "def choose_random_city():\n",
    "    return random.choice(geocode_list)\n",
    "\n",
    "# generate only 1000 driver/rider names to generate more overlapping data\n",
    "names_list = []\n",
    "for i in range(1000):\n",
    "    names_list.append(names.get_full_name())\n",
    "\n",
    "def choose_random_name():\n",
    "    return random.choice(names_list)\n",
    "    \n",
    "# Generation of License Plate\n",
    "# create a pool of letters to choose from\n",
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "numbers = '0123456789'\n",
    "\n",
    "def generate_license_plate():\n",
    "    # generate 3 randomly chosen letters, L1, L2, L3\n",
    "    L1 = random.choice(letters)\n",
    "    L2 = random.choice(letters)\n",
    "    L3 = random.choice(letters)\n",
    "    L4 = random.choice(letters)\n",
    "    # generate 4 randomly chosen numbers, N1, N2, N3, N4\n",
    "    N1 = random.choice(numbers)\n",
    "    N2 = random.choice(numbers)\n",
    "  \n",
    "    # combine it together into one print function\n",
    "    return(L1+L2+'-'+L3+L4+'-'+N1+N2)\n",
    "\n",
    "# Calculation of price based on distance between start city and end destination\n",
    "def calculate_price(v_distance):\n",
    "    v_multiplicator=round(random.uniform(0.8, 2.0),2)\n",
    "    v_price=round(v_distance*v_multiplicator,2)\n",
    "    return(v_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-commission",
   "metadata": {},
   "source": [
    "Let's generate our sample dataset, containing about 300.000 records in total, in order to demonstrate the different Pinot concepts and mechanisms later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin generating trips data at current time\n",
    "start_timestamp_ms = time.time_ns() // 1000000\n",
    "\n",
    "# Generate data\n",
    "num_records = 300000 + random.randint(5000,10000)\n",
    "for i in range(num_records):\n",
    "    v_start_location = choose_random_city()\n",
    "    v_end_location = choose_random_city()\n",
    "    v_distance = random.randint(5,1000)\n",
    "\n",
    "    # add random jitter, in large system our event stream is probably also not strictly sorted\n",
    "    v_requesttime = start_timestamp_ms + i*1000 + random.randint(0,100);\n",
    "\n",
    "    v_waiting_time_millis = random.randint(1,3600000)\n",
    "    v_trip_time = round((v_distance/random.randint(45,60)) * 60 *60*1000)\n",
    "\n",
    "    record = {\n",
    "        \"rider_name\": choose_random_name(),\n",
    "        \"driver_name\": choose_random_name(),\n",
    "        \"license_plate\": generate_license_plate(),\n",
    "        \"start_location\": v_start_location[2],\n",
    "        \"start_zip_code\": v_start_location[1],\n",
    "        \"start_location_state\": v_start_location[3],\n",
    "        \"end_location\": v_end_location[2],\n",
    "        \"end_zip_code\": v_end_location[1],\n",
    "        \"end_location_state\": v_end_location[3],\n",
    "        \"rider_is_premium\": random.randint(0, 1),\n",
    "        \"count\": 1,\n",
    "        \"payment_amount\": calculate_price(v_distance),\n",
    "        \"payment_tip_amount\": random.randint(5,50),\n",
    "        \"trip_wait_time_millis\": v_waiting_time_millis,\n",
    "        \"rider_rating\": random.randint(0,5),\n",
    "        \"driver_rating\": random.randint(0,5),\n",
    "        \"trip_start_time_millis\": v_requesttime + v_waiting_time_millis,\n",
    "        \"request_time_millis\": v_requesttime,\n",
    "        \"trip_end_time_millis\": v_requesttime + v_waiting_time_millis + v_trip_time\n",
    "    }\n",
    " \n",
    "    producer.send('trips', value=record)\n",
    "        \n",
    "    if i % 5000 == 0:\n",
    "        print(f'{i} records generated')\n",
    "\n",
    "print(f'done generating {num_records} records, ready to do some fancy analytics!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-planning",
   "metadata": {},
   "source": [
    "### Tables\n",
    "\n",
    "Tables represent a collection of related data in Pinot. A table either have the type `OFFLINE` (ingesting pre-built pinot-segments from external stores) or `REALTIME` (data ingestion from streams). The user is not required to know the type of a table when querying it.\n",
    "\n",
    "To configure a table, properties like name, type and indexing are required. In the following example, we create an example table which is consuming data from the Kafka topic filled above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "described-mortgage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'status': 'Table trips_REALTIME succesfully added'}\n"
     ]
    }
   ],
   "source": [
    "json_tableConfig = {\n",
    "  \"tableName\": \"trips\",\n",
    "  \"tableType\": \"REALTIME\",\n",
    "  \"segmentsConfig\": {\n",
    "    \"timeColumnName\": \"trip_start_time_millis\",\n",
    "    \"timeType\": \"MILLISECONDS\",\n",
    "    \"retentionTimeUnit\": \"DAYS\",\n",
    "    \"retentionTimeValue\": \"60\",\n",
    "    \"schemaName\": \"trips\",\n",
    "    \"replication\": \"1\",\n",
    "    \"replicasPerPartition\": \"1\"\n",
    "  },\n",
    "  \"tenants\": {},\n",
    "  \"tableIndexConfig\": {\n",
    "    \"loadMode\": \"MMAP\",\n",
    "    \"streamConfigs\": {\n",
    "      \"streamType\": \"kafka\",\n",
    "      \"stream.kafka.consumer.type\": \"simple\",\n",
    "      \"stream.kafka.topic.name\": \"trips\",\n",
    "      \"stream.kafka.decoder.class.name\": \"org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder\",\n",
    "      \"stream.kafka.consumer.factory.class.name\": \"org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory\",\n",
    "      \"stream.kafka.zk.broker.url\": \"pinot-kafka-zookeeper:2181\",\n",
    "      \"stream.kafka.broker.list\": \"pinot-kafka:9092\",\n",
    "      \"realtime.segment.flush.threshold.time\": \"12h\",\n",
    "      \"realtime.segment.flush.threshold.size\": \"20000\",\n",
    "      \"stream.kafka.consumer.prop.auto.offset.reset\": \"smallest\"\n",
    "    },\n",
    "      \"noDictionaryColumns\": [\"count\", \"payment_amount\", \"payment_tip_amount\",\"trip_wait_time_millis\", \"rider_rating\", \"driver_rating\"],\n",
    "      \"aggregateMetrics\": True,\n",
    "  },\n",
    "  \"metadata\": {\n",
    "    \"customConfigs\": {}\n",
    "  }\n",
    "} \n",
    "\n",
    "response = requests.post('http://pinot-controller.pinot:9000/tables', json=json_tableConfig)\n",
    "print(response)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-sullivan",
   "metadata": {},
   "source": [
    "After creation, data records of the Kafka Topic are loaded into the table. To execute a query, the SQL statement is sent to the broker of the Pinot cluster. The response contains the result records, as well as query statistics of the execution.\n",
    "\n",
    "While our data is loading, let's query the example table to figure out, how many trips have already been completed with passengers, that are premium members of our ride sharing platform:\n",
    "\n",
    "XXX use helper funcs, show query statistics  \n",
    "XXX execute multiple times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "juvenile-snake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'resultTable': {'dataSchema': {'columnNames': ['trips_count'], 'columnDataTypes': ['DOUBLE']}, 'rows': [[157457.0]]}, 'exceptions': [], 'numServersQueried': 1, 'numServersResponded': 1, 'numSegmentsQueried': 16, 'numSegmentsProcessed': 16, 'numSegmentsMatched': 16, 'numConsumingSegmentsQueried': 1, 'numDocsScanned': 157457, 'numEntriesScannedInFilter': 315170, 'numEntriesScannedPostFilter': 157457, 'numGroupsLimitReached': False, 'totalDocs': 315170, 'timeUsedMs': 17, 'segmentStatistics': [], 'traceInfo': {}, 'minConsumingFreshnessTimeMs': 1618137855289}\n"
     ]
    }
   ],
   "source": [
    "print(requests.post('http://pinot-broker.pinot:8099/query/sql', json={\n",
    "    \"sql\" : \"SELECT SUM(count) as trips_count FROM trips WHERE rider_is_premium = 1\"\n",
    "}).json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-father",
   "metadata": {},
   "source": [
    "### XXX Pre-aggregation?\n",
    "\n",
    "One feature of Pinot, contributing to performance improvements, is the pre-aggregation for rows having the same dimension values. With that, realtime stream data is aggregated when its consume to reduce segment sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-federation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
